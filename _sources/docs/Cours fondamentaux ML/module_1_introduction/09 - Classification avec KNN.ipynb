{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82ca8c6f-d2dd-4701-940a-96d5cb87a824",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import plotly.io as pio\n",
    "from plotly.io._base_renderers import IFrameRenderer\n",
    "\n",
    "\n",
    "class CumulIFrameRenderer(IFrameRenderer):\n",
    "    def build_filename(self):\n",
    "        max_number = 0\n",
    "\n",
    "        if not os.path.exists(self.html_directory):\n",
    "            os.mkdir(self.html_directory)\n",
    "\n",
    "        for file in os.listdir(self.html_directory):\n",
    "            number = file.split('_')[1]\n",
    "            number = number.split('.')[0]\n",
    "            number = int(number)\n",
    "            max_number = max(max_number, number)\n",
    "\n",
    "        return os.path.join(self.html_directory, f'figure_{max_number+1}.html')\n",
    "\n",
    "\n",
    "pio.renderers['iframe'] = CumulIFrameRenderer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704cad4f-142b-44d3-a112-91658c87a3be",
   "metadata": {
    "id": "704cad4f-142b-44d3-a112-91658c87a3be",
    "tags": []
   },
   "source": [
    "# Classification avec KNN\n",
    "\n",
    "Si vous vous rapplez du chapitre 2, le Machine Learning peut être utilisé pour deux tâches: la régression et la classification. Nous avons introduit la régression dans le chapitre précédent, il est donc temps de présenter la classification. \n",
    "\n",
    "Comme son nom l'indique, le but est de catégoriser des points de données comme étant malades ou sains, un chiffre entre 0 et 9, des bus, avions, etc. La différence principale entre la régression et la classification est que le modèle prédit des entiers et non des nombres réels.\n",
    "\n",
    "Comme pour la régression, beaucoup de modèles existent pour la classification tel que les arbres, la régression logistique et même les réseaux neuronaux. Cependant, ce chapitre va se concentrer sur K-Nearest Neighbours (KNN) car c'est le modèle le plus visuel et simple. Il est aussi relativement performant et est modulable pour s'adapter au problème. \n",
    "\n",
    "\n",
    "## K-Nearest Neighbours\n",
    "\n",
    "Ce modèle part du principe que les données proches dans un espace forment un groupe qui appartient à une certaine catégorie. Le but est simple, nous devons trouver les points les plus proches de celui que nous souhaitons classifier et mettre leurs catégories en commun. Par exemple, on peut classifier un point comme étant de la classe la plus présente dans ces voisins. C'est cette approche qui est utilisé dans le gif qui suit. \n",
    "\n",
    ":::{figure-md} fonc_-fig\n",
    "<img src=\"src/knn_gif.gif\" alt=\"Fonctionnement de Knn\" width=\"750\"/>\n",
    "\n",
    "Fonctionnement de KNN\n",
    ":::\n",
    "\n",
    "Les lignes représentent les voisins sélectionné et leur catégorie. Vu que nous avons 4 points bleu et 1 point orange, notre point devient bleu. Ce GIF présente KNN avec 5 voisins, mais pourquoi 5 et pas 1 ou 10? De plus, je parle de distance mais comment est-elle calculée?\n",
    "C'est sur ces deux points que la prochaine partie va se concentrer.\n",
    "\n",
    "## Mesure de distance\n",
    "Nous pouvons utiliser plusieurs méthodes de mesure différentes comme la mesure Euclidienne (l2-norm), celle de Manhattan (l1-norm), et d'autres. Le mesure la plus simple est la distance euclidienne qui calcule la distance dans l'espace cartésien.\n",
    "\n",
    "$$distance(x_{new},x_n)=\\sqrt{\\sum_{a=1}^A(x_{new,a}-x_{n,a})^2}$$\n",
    "$A$ représente tous les attributs.\n",
    "\n",
    "Par facilité nous pouvons omettre la racine carrée et réécrire la formule en utilisant des vecteurs:\n",
    "\n",
    "$$\\begin{align}\n",
    "    distance(x_{new},x_n)^2 &= \\sum_{a=1}^A(x_{new,a}-x_{n,a})^2 \\\\\n",
    "    &= \\sum_{a=1}^A(x_{new,a}-x_{n,a})^T(x_{new,a}-x_{a,d}) \\\\\n",
    "    &= (x_{new}-x_n)^T(x_{new}-x_n)\n",
    "\\end{align}$$\n",
    "\n",
    "Cette formulation est la plus répendue et est pratique car les ordinateurs sont particulièrement performants pour le calcul vectoriel. Il est important de noter que $T$ représente la transposée. \n",
    "\n",
    "Imaginons que nous voulons calculer la distance entre un point $x_{new}=\\begin{bmatrix} 0.5 \\\\ 0.5 \\end{bmatrix}$, et deux autres points $x_1= \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$ et $x_2= \\begin{bmatrix} 0.75 \\\\ 0.75 \\end{bmatrix}$.\n",
    "\n",
    "$$\\begin{align}\n",
    "    distance(x_{new}, x_1)^2 &= (x_{new}-x_1)^T(x_{new}-x_1) \\\\\n",
    "        &= \\begin{bmatrix} 0.5 & 0.5 \\end{bmatrix}\\begin{bmatrix} 0.5 \\\\ 0.5 \\end{bmatrix} = 0.25+0.25 = 0.5 \\\\\n",
    "    distance(x_{new}, x_2)^2 &= (x_{new}-x_2)^T(x_{new}-x_2) \\\\\n",
    "        &= \\begin{bmatrix} -0.25 & -0.25 \\end{bmatrix}\\begin{bmatrix} -0.25 \\\\ -0.25 \\end{bmatrix} = 0.0625 + 0.0625 = 0.125\n",
    "\\end{align}$$\n",
    "\n",
    "Le point $x_2$ est donc le plus proche de notre point.\n",
    "\n",
    "\n",
    "Il est aussi possible de calculer la distance dans un autre espace grâce aux kernels. Cependant, cette méthode ne sera pas expliqué dans ce module. \n",
    "\n",
    "> Vous aurez remarqué que la distance d'un point s'évalue sur base de la différence entre chaque attribut. C'est pour cette raison qu'il faut de préférence normaliser les données quand on utilise KNN. Imaginons que nous avons un attributs dont les valeurs se trouvent entre 0 et 1, et un autre entre -100 et 100. Le second attribut aura plus de poids dans la mesure de distance car il aura de plus grande valeurs comparé au premier. \n",
    "Par exemple nous avons deux point; (0.5, 50) et (0.45, 0). La distance entre les deux premier attributs est de $(0.5-0.45)²=0.0025$ et entre les deuxièmes $(50-0)²=2500$. Ils auront donc une distance de 2500.0025 entre les deux alors qu'ils sont très proche dans le premier attribut. \n",
    "La normalisation est encore plus importante quand on décide d'utiliser des poids lors de l'aggrégation.\n",
    "\n",
    "## Nombre de voisins\n",
    "\n",
    "Maintenant que nous savons comment la distance sera mesurée il faut savoir combien de voisins nous devons prendre en compte. Pour voir les délimitations (bordures de décisions) entre 2 classes, nous pouvons créer un graphiques comme celui qui suit. Ce graphique présente les deux catégories ainsi que la bordure de décision. \n",
    "\n",
    ":::{figure-md} ks-fig\n",
    "<img src=\"src/bordure_decision_knn.svg\" alt=\"Influence du nombre de voisins sur la bordure de décision\" width=\"750\"/>\n",
    "\n",
    "Influence du nombre de voisins sur la bordure de décision\n",
    ":::\n",
    "\n",
    "Nous pouvons voir que si nous utilisons 1 voisin, nous avons une poche dans les bleus où le point sera classifié comme orange. Utiliser 5 voisins semble suivre la forme de nos données alors qu'en utiliser plus linéarise la bordure. Bien que la meilleur façon de choisir le nombre de voisins en utilisants des mesures tel que l'accuracy ou le F1-score, créer un graphique nous permet de visualier la bordure et de comprendre le modèle.   \n",
    "\n",
    "Il est dont primordial de déterminer le nombre optimal de voisins à prendre en compte. Le plus efficace et simple est d'utiliser les données d'entraînement pour trouver ce nombre. Nous pouvons déterminer une liste de nombres des voisins et entraîner chaque version avec l'ensemble d'entraînement et de l'évaluer sur l'ensemble de validation. La version du modèle la plus performante est celle que nous garderons. \n",
    "\n",
    "Un autre facteur déterminant est la balance des données. Si notre dataset d'entraînement n'est pas balancé et que nous avons 10 points qui sont de classe 1 et 90 de classe 2, utiliser un k > 10 va toujours prédire 2. \n",
    "\n",
    "## Agrégation des résultats\n",
    "La dernière chose à déterminer est la façon dont on va aggréger les voisins. KNN est utilisé plus souvent dans les tâches de classification mais ce modèle peut aussi être utilisé pour de la régression.\n",
    "\n",
    "### Classification\n",
    "\n",
    "Le plus simple est d'attribuer la classe qui est la plus présente dans les voisins qui ont été selectionnés. Dans certain cas, on voudra prendre la distance des voisins en compte. En effet, il est possible que sur 5 voisins, notre point soit collé à deux points d'une classe mais qu'il ait 3 autres voisins assez distants. Si nous utilisons la première méthode, le point sera classifié comme appartenant à la même classe que les trois points distant. Ce problème peut-être résolu en prenant la distance en compte. Pour ce faire, on attribue un poids à chaque point. Si l'influence d'un voisin est inversement proportionnelle à sa distance, le nouveau point sera correctement classifié \n",
    "chaque vote, tel que l'inverse de la distance, les deux points proches auront l'avantage et notre nouveau point sera correctement classifié. \n",
    "\n",
    "### Régression\n",
    "Dans le cas de régression, nous prendrons la moyenne de chaque voisins. Dans ce cas nous pouvons aussi prendre la distance en compte pour faire une moyenne pondérée. \n",
    "\n",
    "## Faiblesses du modèle\n",
    "### Risque de sur-apprentissage\n",
    "KNN ne fait que très peu d'hypothèses sur la forme que doit avoir la délimitation entre les classes.\n",
    "Il a très peu de biais, ce qui lui permet de s'adapter à toute forme de délimitation, mais cela est aussi sa grosse faiblesse car il est très facile de sur-apprendre.\n",
    "Lors de son utilisation, paufinez avec minutie le nombre de voisin pris en compte car c'est cela qui permet de régulariser la prédiction.\n",
    "\n",
    "### Stockage des données & temps d'inférence\n",
    "Pour que l'algorithme fonctionne bien, il faut stocker les données d'entraînement puisque c'est sur celles-ci que sont comparé les nouveaux points lors d'une prédiction.\n",
    "De même, vous aurez besoin de calculer la distance de chaque nouveau point à classifier avec tous les points d'entraînement, ce qui peut être lent.\n",
    "Vous l'aurez compris, si votre jeu de données est trop gros, KNN ne sera peut-être pas votre meilleure option.\n",
    "\n",
    "Des méthodes existent pour contrer cela, en ne gardant par exemple uniquement les points représentatifs parmis ceux d'entraînement (voir le *Clustering*).\n",
    "\n",
    "### La malédiction de la dimensionnalité\n",
    "Nous savons que dans le cadre de la classification, KNN part du principe que les échantillons d'une même catégorie sont proches les uns des autres et que, idéalement, les données d'entraînement sont équilibrée. Un point important à prendre en compte est ce que l'on appelle \"la malédiction de la dimensionnalité\". Le volume d'une espace de données augmente exponentiellement avec le nombre d'attributs. Ainsi nous avons besoin d'une quantité exponentielle d'échantillons, sinon nous aurons des espaces quasi-vides et la distance entre chaque échantillon sera trop similaire. KNN évalue la distance entre un point et un autre en fonction de la distance dans chaque attribut ($(x_{new,a}-x_{n,a})^2$), il sera donc de plus en plus difficile pour KNN de trouver les voisins correctement.\n",
    "\n",
    "## À retenir\n",
    "\n",
    "- Lors de la classification, KNN part du principe que les échantillons de la même catégorie sont proches l'un de l'autre.\n",
    "- KNN est un modèle assez simple mais qui est modulable afin de correspondre le plus possible à notre tâche.\n",
    "- Il faut faire attention à l'équilibre de l'ensemble d'entraînement. \n",
    "- Ce modèle peut-être utilisé pour des tâches de classification ainsi que de régression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d62b4f8-84ea-4252-844d-aab71ff2b406",
   "metadata": {
    "id": "0d62b4f8-84ea-4252-844d-aab71ff2b406"
   },
   "source": [
    "## Implémentation\n",
    "\n",
    "Pour faciliter l'implémentation nous allons utiliser numpy. Nous allons aussi suivre l'architecture de scikit learn (Sklearn), c'est à dire que notre modèle possède une fonction **.fit** pour l'entraîner et **.predict** pour la prédiction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cab3510-52cf-4c4a-af98-f8b5df806539",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2cab3510-52cf-4c4a-af98-f8b5df806539",
    "outputId": "3efb8e15-5536-4f3a-cf23-904c9c0fdef1"
   },
   "outputs": [],
   "source": [
    "# Pour le modèle\n",
    "import numpy as np\n",
    "\n",
    "# Pour le plot\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "'''\n",
    "-- Création de l'ensemble de données --\n",
    "Notez que j'utilise une distribution normale ce qui permet d'avoir\n",
    "tous les points d'une même classe relativement proches.\n",
    "De plus, je génère un ensemble équilibré.\n",
    "'''\n",
    "\n",
    "elements = 50\n",
    "random_state = 42\n",
    "K = 5\n",
    "\n",
    "def generate_data(elements, random_state):\n",
    "    rng=np.random.RandomState(random_state)\n",
    "\n",
    "    classes_x = rng.normal(0, 0.55, (2,elements))\n",
    "    classes_y = rng.normal(1, 0.55, (2,elements))\n",
    "    labels = np.concatenate((np.zeros(elements), np.ones(elements)))\n",
    "    dset = np.zeros((2,elements*2))\n",
    "\n",
    "    dset[0] = np.concatenate((classes_x[0], classes_y[0]))\n",
    "    dset[1] = np.concatenate((classes_x[1], classes_y[1]))\n",
    "\n",
    "    return dset, labels\n",
    "\n",
    "'''\n",
    "-- Implémentation du modèle --\n",
    "'''\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, K):\n",
    "        self.K = K\n",
    "    \n",
    "\n",
    "    def fit(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    \n",
    "    def predict(self, data_point):\n",
    "        preds = []\n",
    "\n",
    "        for dp in data_point:\n",
    "            distances = []\n",
    "\n",
    "            for point in self.data:\n",
    "                distances.append(self.distance(dp, point)) \n",
    "\n",
    "            sorted_distances = np.argsort(distances)\n",
    "\n",
    "            preds.append(np.bincount(self.labels[sorted_distances[:self.K]].astype(int)).argmax())\n",
    "        \n",
    "        return np.array(preds)\n",
    "    \n",
    "\n",
    "    def distance(self, x_new, x):\n",
    "        return np.dot(x_new-x, x_new-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3dba1c-380f-4bdf-ac45-de5c7d8bb13f",
   "metadata": {},
   "source": [
    "Maintenant que nous avons un modèle et une méthode de génération de données, nous pouvons l'utiliser pour faire des prédictions. Nous allons générer 10 points de chaque classe. Vu que nous avons un ensemble équilibré et binaire, nous pouvons utiliser l'accuracy. Cette mesure calcule le pourcentage de mauvaise prédiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddb5f10c-7ead-44e2-92d1-b4f88d4f3d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "# On génère les données d'entraînement et on \"fit\" le modèle\n",
    "train_set, train_labels = generate_data(elements, random_state)\n",
    "\n",
    "# Ensuite on le teste en générant le set de test \n",
    "elements = 10\n",
    "test_set, test_labels = generate_data(elements, random_state)\n",
    "    \n",
    "model = KNN(5)\n",
    "model.fit(train_set.T, train_labels)\n",
    "\n",
    "preds = model.predict(test_set.T)\n",
    "\n",
    "print(np.sum(preds != test_labels)/len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b6c59e-033e-4d46-b9cf-80aac193deeb",
   "metadata": {},
   "source": [
    "Nous avons deux erreurs sur les 20 points, ce qui donne une accuracy de 0.1. C'est plutôt pas mal. Pour comprendre d'où elles viennent, nous pouvons tracer la bordure avec les points d'entraînements et de test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fTJDhcDTmS09",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "fTJDhcDTmS09",
    "outputId": "d546c298-9fd4-44fa-b159-52212d6177e0"
   },
   "outputs": [],
   "source": [
    "# Code utilisé pour générer l'image\n",
    "\n",
    "# Création du mesh pour voir la bordure de décision\n",
    "mesh_size = .02\n",
    "margin = 0.25\n",
    "\n",
    "# Create a mesh grid on which we will run our model\n",
    "x_min, x_max = train_set[0].min() - margin, train_set[0].max() + margin\n",
    "y_min, y_max = train_set[1].min() - margin, train_set[1].max() + margin\n",
    "xrange = np.arange(x_min, x_max, mesh_size)\n",
    "yrange = np.arange(y_min, y_max, mesh_size)\n",
    "xx, yy = np.meshgrid(xrange, yrange)\n",
    "\n",
    "# Les couleurs pour la bordure\n",
    "color_scale = [[0, \"rgb(255, 255, 255)\"], [1, \"rgb(122, 122, 122)\"]]\n",
    "\n",
    "preds = model.predict(test_set.T)\n",
    "preds[preds != test_labels] = -1    \n",
    "\n",
    "Z = np.array(model.predict(np.c_[xx.ravel(), yy.ravel()])).astype(str)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Contour(\n",
    "        x=xrange,\n",
    "        y=yrange,\n",
    "        z=Z,\n",
    "        colorscale=color_scale,\n",
    "        showscale=False\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=train_set[0][train_labels == 0], y=train_set[1][train_labels==0], mode=\"markers\",\n",
    "      marker=dict(color=\"rgb(2,48,71)\"), name=\"entraînement: 0\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=train_set[0][train_labels == 1], y=train_set[1][train_labels==1], mode=\"markers\",\n",
    "      marker=dict(color=\"rgb(255,158,2)\"), name=\"entraînement: 1\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=test_set[0][preds == 0], y=test_set[1][preds==0], mode=\"markers\",\n",
    "      marker=dict(color=\"rgb(28,128,178)\", size=7.5, line=dict(width=2, color='DarkSlateGrey')),\n",
    "                  name=\"Correctement Prédit comme 0\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=test_set[0][preds == 1], y=test_set[1][preds==1], mode=\"markers\",\n",
    "      marker=dict(color=\"rgb(176,78,68)\", size=7.5, line=dict(width=2, color='DarkSlateGrey')),\n",
    "                  name=\"Correctement Prédit comme 1\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=test_set[0][preds == -1], y=test_set[1][preds==-1], mode=\"markers\",\n",
    "      marker=dict(color=\"rgb(255,0,0)\", size=10, line=dict(width=2, color='DarkSlateGrey')),\n",
    "                  name=\"Erreur Prédit comme 0 au lieu de 1\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(height=800, width=800)\n",
    "fig.show('iframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d99aa79-6ba1-487f-8502-e0d009311691",
   "metadata": {},
   "source": [
    "Nous pouvons voir que que les deux erreurs sont deux points qui sont de classe 1 mais prédits de classe 0 par le modèle.\n",
    "Même certains points du dataset d'entraînement ne sont pas tous bien classifier par le modèle (ceux du mauvais côté de la bordure de décision).\n",
    "On peut toujours adapter le nombre de voisins afin de réduire l'erreur mais il faut faire attention à l'overfitting !\n",
    "\n",
    "## Sources\n",
    "- Cours \"Algorithms for Machine Learning and Inference\" de Chalmers University of Technology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bd89fb-0c08-475c-a785-4ff8afdb2d5f",
   "metadata": {
    "id": "15bd89fb-0c08-475c-a785-4ff8afdb2d5f"
   },
   "source": [
    "<!--\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "h = .02\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "random_state = 42\n",
    "rng=np.random.RandomState(random_state )\n",
    "\n",
    "x1 = rng.normal(0, 0.55, 50)\n",
    "y1 = rng.normal(0, 0.55, 50)\n",
    "\n",
    "x2 = rng.normal(1, 0.55, 50)\n",
    "y2 = rng.normal(1, 0.55, 50)\n",
    "\n",
    "x = np.concatenate((x1, x2))\n",
    "y = np.concatenate((y1, y2))\n",
    "labels = np.concatenate((np.full(len(x1), 0), np.full(len(x2), 1)))\n",
    "\n",
    "f, axis = plt.subplots(1, 4, sharey=True, figsize=(15,15))\n",
    "\n",
    "line = 0\n",
    "\n",
    "for ind, k in enumerate([1, 5, 10, 15]):\n",
    "    \n",
    "    if ind > 3:\n",
    "        ind -= 4\n",
    "        line = 1\n",
    "        \n",
    "    X = np.transpose(np.array([x,y]))\n",
    "                        \n",
    "    kNN = KNeighborsClassifier(k)\n",
    "    kNN.fit(X, labels)\n",
    "\n",
    "    np.transpose(X)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    Z = kNN.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    #plt.figure()\n",
    "    axis[ind].pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "    # Plot also the training points\n",
    "    axis[ind].scatter(X[:,0], X[:,1], c=labels, cmap=cmap_bold)\n",
    "    axis[ind].set_xlim(xx.min(), xx.max())\n",
    "    axis[ind].set_ylim(yy.min(), yy.max())\n",
    "    axis[ind].set(aspect=1)\n",
    "    axis[ind].set_title(f\"k={k}\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig(\"choix_des_k\", bbox_inches='tight', pad_inches=0)\n",
    "plt.show()\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "cours_knn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
