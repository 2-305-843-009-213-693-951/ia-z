{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e173effd-4a40-46d6-ac04-5fbb2bea0f0f",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import plotly.io as pio\n",
    "from plotly.io._base_renderers import IFrameRenderer\n",
    "\n",
    "\n",
    "class CumulIFrameRenderer(IFrameRenderer):\n",
    "    def build_filename(self):\n",
    "        max_number = 0\n",
    "\n",
    "        if not os.path.exists(self.html_directory):\n",
    "            os.mkdir(self.html_directory)\n",
    "\n",
    "        for file in os.listdir(self.html_directory):\n",
    "            number = file.split('_')[1]\n",
    "            number = number.split('.')[0]\n",
    "            number = int(number)\n",
    "            max_number = max(max_number, number)\n",
    "\n",
    "        return os.path.join(self.html_directory, f'figure_{max_number+1}.html')\n",
    "\n",
    "\n",
    "pio.renderers['iframe'] = CumulIFrameRenderer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704cad4f-142b-44d3-a112-91658c87a3be",
   "metadata": {
    "id": "704cad4f-142b-44d3-a112-91658c87a3be",
    "tags": [
     "remove-input"
    ]
   },
   "source": [
    "# Classification: K-nearest-neighbours\n",
    "\n",
    "Si vous vous rappelez du chapitre 2, le Machine Learning peut être utilisé pour deux tâches: la régression et la classification. Nous avons introduit la régression dans le chapitre précédent, il est donc temps de présenter la classification. \n",
    "L'approche la plus simple et intuitive est de partir du principe que les points de données de même catégorie se ressemblent. En partant de cette hypothèse, nous pouvons créer un modèle qui va comparer le point que nous souhaitons prédire ($x_{new}$) avec ceux qui sont déjà connus et aggréger les plus proches. \n",
    "C'est exactement ce que fait l'algorithme K-nearest-neighbours (\"k voisins les plus proches\"). Il va d'abord stocker les données d'entraînement avec leurs labels respectifs. Ensuite, lors de la prédiction, il va trouver les $k$ voisins les plus proches et aggréger leur labels respectifs.\n",
    "\n",
    "Il faudra résoudre plusieurs problèmes avant de pouvoir utiliser KNN. En effet, il faut déterminer le nombre de voisins ($k$) à prendre en compte, la façon de calculer les distances ainsi que la façon d'aggréger les voisins.\n",
    "\n",
    "## Mesure de Distance\n",
    "Nous pouvons utiliser plusieurs méthodes de mesure différentes comme la mesure Euclidienne (l2-norm), celle de Manhattan (l1-norm), et d'autres. Le mesure la plus simple est la distance euclidienne qui calcule la distance dans l'espace cartésien.\n",
    "\n",
    "$$distance(x_{new},x_n)=\\sqrt{\\sum_{a=1}^A(x_{new,a}-x_{n,a})^2}$$\n",
    "$A$ représente tous les attributs.\n",
    "\n",
    "Par facilité nous pouvons omettre la racine carrée et réécrire la formule en utilisant des vecteurs:\n",
    "\n",
    "$$\\begin{align}\n",
    "    distance(x_{new},x_n)^2 &= \\sum_{a=1}^A(x_{new,a}-x_{n,a})^2 \\\\\n",
    "    &= \\sum_{a=1}^A(x_{new,a}-x_{n,a})^T(x_{new,a}-x_{a,d}) \\\\\n",
    "    &= (x_{new}-x_n)^T(x_{new}-x_n)\n",
    "\\end{align}$$\n",
    "\n",
    "Cette formulation est la plus répendue et est pratique car les ordinateurs sont particulièrement performants pour le calcul vectoriel.\n",
    "\n",
    "Il est aussi possible de calculer la distance dans un autre espace grâce aux kernels. Cependant, cette méthode ne sera pas expliqué dans ce module. \n",
    "\n",
    "> Vous aurez remarqué que la distance d'un point s'évalue sur base de la différence entre chaque attribut.\n",
    "C'est pour cette raison qu'il faut de préférence normaliser les données quand on utilise KNN.\n",
    "Imaginons que nous avons un attributs dont les valeurs se trouvent entre 0 et 1, et un autre entre -100 et 100.\n",
    "Le second attribut aura plus de poids dans la mesure de distance car il aura de plus grande valeurs comparé au premier. \n",
    "Par exemple nous avons deux point; (0.5, 50) et (0.45, 0).\n",
    "La distance entre les deux premier attributs est de $(0.5-0.45)²=0.0025$ et entre les deuxièmes $(50-0)²=2500$.\n",
    "Ils auront donc une distance de 2500.0025 entre les deux alors qu'ils sont très proche dans le premier attribut.\n",
    "La normalisation est encore plus importante quand on décide d'utiliser des poids lors de l'aggrégation.\n",
    "\n",
    "## Nombre de voisins\n",
    "\n",
    "Maintenant que nous savons comment la distance sera mesurée il faut savoir combien de voisins nous devons prendre en compte. Pour voir les délimitations (bordures de décisions) entre 2 classes, nous pouvons créer un graphiques comme celui qui suit. Ce graphique présente les deux catégories ainsi que la bordure de décision. \n",
    "\n",
    ":::{figure-md} ks-fig\n",
    "<img src=\"src/ks.png\" alt=\"Influence du nombre de voisins sur la bordure de décision\" width=\"750\"/>\n",
    "\n",
    "Influence du nombre de voisins sur la bordure de décision\n",
    ":::\n",
    "\n",
    "Nous pouvons voir que si nous utilisons 1 voisin, nous avons une poches dans les bleus où le point sera classifié comme orange. Utiliser 5 voisins semble suivre la forme de nos données alors qu'en utiliser plus linéarise la bordure. Bien que la meilleur façon de choisir le nombre de voisins en utilisants des mesures tel que l'accuracy ou le F1-score, créer un graphique nous permet de visualier la bordure et de comprendre le modèle.   \n",
    "\n",
    "\n",
    "\n",
    "**Une image est présente pour montrer l'impact du nombre de voisins**\n",
    "\n",
    "Il est dont primordial de déterminer le nombre optimal de voisins à prendre en compte. Le plus efficace et simple est d'utiliser les données d'entraînement pour trouver ce nombre. Nous pouvons déterminer une liste de nombres des voisins et entraîner chaque version avec l'ensemble d'entraînement et de l'évaluer sur l'ensemble de validation. La version du modèle la plus performante est celle que nous garderons. \n",
    "\n",
    "Un autre facteur déterminant est la balance des données. Si notre dataset d'entraînement n'est pas balancé et que nous avons 10 points qui sont de classe 1 et 90 de classe 2, utiliser un k > 10 va toujours prédire 2. \n",
    "\n",
    "## Agrégation des résultats\n",
    "La dernière chose à déterminer est la façon dont on va aggréger les voisins. KNN est utilisé plus souvent dans les tâches de classification mais ce modèle peut aussi être utilisé pour de la régression.\n",
    "\n",
    "### Classification\n",
    "\n",
    "Le plus simple est d'attribuer la classe qui est la plus présente dans les voisins qui ont été selectionnés. Dans certain cas, on voudra prendre la distance des voisins en compte. En effet, il est possible que sur 5 voisins, notre point soit collé à deux points d'une classe mais qu'il ait 3 autres voisins assez distants. Si nous utilisons la première méthode, le point sera classifié comme appartenant à la même classe que les trois points distant. Ce problème peut-être résolu en prenant la distance en compte. Pour ce faire, on attribue un poids à chaque point. Si l'influence d'un voisin est inversement proportionnelle à sa distance, le nouveau point sera correctement classifié \n",
    "chaque vote, tel que l'inverse de la distance, les deux points proches auront l'avantage et notre nouveau point sera correctement classifié. \n",
    "\n",
    "### Régression\n",
    "Dans le cas de régression, nous prendrons la moyenne de chaque voisins. Dans ce cas nous pouvons aussi prendre la distance en compte. \n",
    "\n",
    "## Point Important\n",
    "Nous savons que dans le cadre de la classification, KNN part du principe que les échantillons d'une même catégorie sont proches les uns des autres et que, idéalement, les données d'entraînement sont équilibrée. Un autre point important à prendre en compte est ce qu'on appelle \"la malédiction de la dimensionnalité\". Le volume d'une espace de données augumente exponentielement avec le nombre d'attributs, ainsi nous avons besoin d'une quantité exponentielle d'échantillons, sinon nous aurons des espaces quasi vides et la distance entre chaque échantillon augmentera. KNN évalue la distance entre un point et un autre en fonction de la distance dans chaque attribut ($(x_{new,a}-x_{n,a})^2$), il sera donc de plus en plus difficile pour KNN de trouver les voisins correctement. \n",
    "\n",
    "## À retenir\n",
    "\n",
    "- Lors de la classification, KNN part du principe que les échantillons de la même catégorie sont proches l'un de l'autre.\n",
    "- KNN est un modèle assez simple mais qui est modulable afin de correspondre le plus possible à notre tâche.\n",
    "- Il faut faire attention à l'équilibre de l'ensemble d'entraînement.\n",
    "- Ce modèle peut-être utilisé pour des tâches de classification ainsi que de régression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d62b4f8-84ea-4252-844d-aab71ff2b406",
   "metadata": {
    "id": "0d62b4f8-84ea-4252-844d-aab71ff2b406"
   },
   "source": [
    "## Implémentation\n",
    "\n",
    "Pour faciliter l'implémentation nous allons utiliser numpy. Nous allons aussi suivre l'architecture de scikit learn (Sklearn), c'est à dire que notre modèle possède une fonction **.fit** pour l'entraîner et **.predict** pour la prédiction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cab3510-52cf-4c4a-af98-f8b5df806539",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2cab3510-52cf-4c4a-af98-f8b5df806539",
    "outputId": "3efb8e15-5536-4f3a-cf23-904c9c0fdef1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "'''\n",
    "-- Création de l'ensemble de données --\n",
    "Notez que j'utilise une distribution normale ce qui permet d'avoir tous les points d'une même classe relativement proches.\n",
    "De plus, je génère un ensemble équilibré.\n",
    "'''\n",
    "\n",
    "elements = 50\n",
    "random_state = 42\n",
    "K = 5\n",
    "\n",
    "def generate_data(elements, random_state):\n",
    "    rng=np.random.RandomState(random_state)\n",
    "\n",
    "    classes_x = rng.normal(0, 0.55, (2,elements))\n",
    "    classes_y = rng.normal(1, 0.55, (2,elements))\n",
    "    labels = np.concatenate((np.zeros(elements), np.ones(elements)))\n",
    "    dset = np.zeros((2,elements*2))\n",
    "\n",
    "    dset[0] = np.concatenate((classes_x[0], classes_y[0]))\n",
    "    dset[1] = np.concatenate((classes_x[1], classes_y[1]))\n",
    "\n",
    "    return dset, labels\n",
    "\n",
    "'''\n",
    "-- Implémentation du modèle --\n",
    "'''\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, K):\n",
    "        self.K = K\n",
    "    \n",
    "\n",
    "    def fit(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    \n",
    "    def predict(self, data_point):\n",
    "        preds = []\n",
    "\n",
    "        for dp in data_point:\n",
    "            distances = []\n",
    "\n",
    "            for point in self.data:\n",
    "                distances.append(self.distance(dp, point)) \n",
    "\n",
    "            sorted_distances = np.argsort(distances)\n",
    "\n",
    "            preds.append(np.bincount(self.labels[sorted_distances[:self.K]].astype(int)).argmax())\n",
    "        \n",
    "        return np.array(preds)\n",
    "    \n",
    "\n",
    "    def distance(self, x_new, x):\n",
    "        return np.power(np.dot(x_new-x, x_new-x), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3dba1c-380f-4bdf-ac45-de5c7d8bb13f",
   "metadata": {},
   "source": [
    "Maintenant que nous avons un modèle et une méthode de génération de données, nous pouvons l'utiliser pour faire des prédictions. Nous allons générer 10 points de chaque classe. Vu que nous avons un ensemble équilibré et binaire, nous pouvons utiliser l'accuracy. Cette mesure calcule le pourcentage de mauvaise prédiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddb5f10c-7ead-44e2-92d1-b4f88d4f3d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "# On génère les données d'entraînement et on \"fit\" le modèle\n",
    "train_set, train_labels = generate_data(elements, random_state)\n",
    "\n",
    "# Ensuite on le teste en générant le set de test \n",
    "elements = 10\n",
    "test_set, test_labels = generate_data(elements, random_state)\n",
    "    \n",
    "model = KNN(5)\n",
    "model.fit(train_set.T, train_labels)\n",
    "\n",
    "preds = model.predict(test_set.T)\n",
    "\n",
    "print(np.sum(preds != test_labels)/len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913704f9-120b-4f3f-be6f-d83191ff6c33",
   "metadata": {},
   "source": [
    "Nous avons deux erreurs sur les 20 points, ce qui donne une accuracy de 0.1. C'est plutôt pas mal. Pour comprendre d'où elles viennent, nous pouvons tracer la bordure avec les points d'entraînements et de test. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bd89fb-0c08-475c-a785-4ff8afdb2d5f",
   "metadata": {
    "id": "15bd89fb-0c08-475c-a785-4ff8afdb2d5f"
   },
   "source": [
    "<!--\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "h = .02\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "random_state = 42\n",
    "rng=np.random.RandomState(random_state )\n",
    "\n",
    "x1 = rng.normal(0, 0.55, 50)\n",
    "y1 = rng.normal(0, 0.55, 50)\n",
    "\n",
    "x2 = rng.normal(1, 0.55, 50)\n",
    "y2 = rng.normal(1, 0.55, 50)\n",
    "\n",
    "x = np.concatenate((x1, x2))\n",
    "y = np.concatenate((y1, y2))\n",
    "labels = np.concatenate((np.full(len(x1), 0), np.full(len(x2), 1)))\n",
    "\n",
    "f, axis = plt.subplots(1, 4, sharey=True, figsize=(15,15))\n",
    "\n",
    "line = 0\n",
    "\n",
    "for ind, k in enumerate([1, 5, 10, 15]):\n",
    "    \n",
    "    if ind > 3:\n",
    "        ind -= 4\n",
    "        line = 1\n",
    "        \n",
    "    X = np.transpose(np.array([x,y]))\n",
    "                        \n",
    "    kNN = KNeighborsClassifier(k)\n",
    "    kNN.fit(X, labels)\n",
    "\n",
    "    np.transpose(X)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    Z = kNN.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    #plt.figure()\n",
    "    axis[ind].pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "    # Plot also the training points\n",
    "    axis[ind].scatter(X[:,0], X[:,1], c=labels, cmap=cmap_bold)\n",
    "    axis[ind].set_xlim(xx.min(), xx.max())\n",
    "    axis[ind].set_ylim(yy.min(), yy.max())\n",
    "    axis[ind].set(aspect=1)\n",
    "    axis[ind].set_title(f\"k={k}\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig(\"choix_des_k\", bbox_inches='tight', pad_inches=0)\n",
    "plt.show()\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fTJDhcDTmS09",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "fTJDhcDTmS09",
    "outputId": "d546c298-9fd4-44fa-b159-52212d6177e0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"820px\"\n",
       "    height=\"820\"\n",
       "    src=\"iframe_figures/figure_8.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the figures\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Création du mesh pour voir la bordure de décision\n",
    "mesh_size = .02\n",
    "margin = 0.25\n",
    "\n",
    "# Create a mesh grid on which we will run our model\n",
    "x_min, x_max = train_set[0].min() - margin, train_set[0].max() + margin\n",
    "y_min, y_max = train_set[1].min() - margin, train_set[1].max() + margin\n",
    "xrange = np.arange(x_min, x_max, mesh_size)\n",
    "yrange = np.arange(y_min, y_max, mesh_size)\n",
    "xx, yy = np.meshgrid(xrange, yrange)\n",
    "\n",
    "# Les couleurs pour la bordure\n",
    "color_scale = [[0, \"rgb(255, 255, 255)\"], [1, \"rgb(122, 122, 122)\"]]\n",
    "\n",
    "preds = model.predict(test_set.T)\n",
    "preds[preds != test_labels] = -1    \n",
    "\n",
    "Z = np.array(model.predict(np.c_[xx.ravel(), yy.ravel()])).astype(str)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Contour(\n",
    "        x=xrange,\n",
    "        y=yrange,\n",
    "        z=Z,\n",
    "        colorscale=color_scale,\n",
    "        showscale=False\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=train_set[0][train_labels == 0], y=train_set[1][train_labels==0], mode=\"markers\",\n",
    "      marker=dict(color=\"rgb(2,48,71)\"), #showlegend = False,\n",
    "       name=\"entraînement: 0\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=train_set[0][train_labels == 1], y=train_set[1][train_labels==1], mode=\"markers\",\n",
    "      marker=dict(color=\"rgb(255,158,2)\"), #showlegend = False,\n",
    "       name=\"entraînement: 1\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=test_set[0][preds == 0], y=test_set[1][preds==0], mode=\"markers\",\n",
    "      marker=dict(color=\"rgb(28,128,178)\", size=7.5, line=dict(width=2, color='DarkSlateGrey')),\n",
    "                  name=\"Correctement Prédit comme 0\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=test_set[0][preds == 1], y=test_set[1][preds==1], mode=\"markers\",\n",
    "      marker=dict(color=\"rgb(176,78,68)\", size=7.5, line=dict(width=2, color='DarkSlateGrey')),\n",
    "                  name=\"Correctement Prédit comme 1\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=test_set[0][preds == -1], y=test_set[1][preds==-1], mode=\"markers\",\n",
    "      marker=dict(color=\"rgb(255,0,0)\", size=10, line=dict(width=2, color='DarkSlateGrey')),\n",
    "                  name=\"Erreur Prédit comme 0 au lieu de 1\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(height=800, width=800, title_text=\"La bordure de décision de notre modèle\")\n",
    "fig.show('iframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d99aa79-6ba1-487f-8502-e0d009311691",
   "metadata": {},
   "source": [
    "Nous pouvons voir que que les deux erreurs sont deux points qui sont de classe 1 mais prédits de classe 0 par le modèle.\n",
    "Même certains points du dataset d'entraînement ne sont pas tous bien classifier par le modèle (ceux du mauvais côté de la bordure de décision).\n",
    "On peut toujours adapter le nombre de voisins afin de réduire l'erreur mais il faut faire attention à l'overfitting !\n",
    "\n",
    "## Sources\n",
    "- Cours \"Algorithms for Machine Learning and Inference\" de Chalmers University of Technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4a3190-dcb1-4881-b6e3-b5b7cf92bce4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "cours_knn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
