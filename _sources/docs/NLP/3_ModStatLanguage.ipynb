{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abcaa4e5",
   "metadata": {},
   "source": [
    "# Chapitre II: Notions générales\n",
    "\n",
    "## Sommaire\n",
    "* [Modélisation statistique du language](#modelisation-statistique-du-language)\n",
    "* [Bag of Words (BoW)](#bag-of-words-bow)\n",
    "   * [Implémentation en Python:](#implementation-en-python)\n",
    "* [Term Frequency-Inverse Document Frenquency (TF-IDF)](#term-frequency-inverse-document-frenquency-tf-idf)\n",
    "   * [Implémentation en Python:](#id1)\n",
    "   \n",
    "## Modélisation statistique du language\n",
    "\n",
    "Maintenant que nous avons vu comment préparer le texte avant son utilisation dans un algorithme de NLP, nous nous intéresserons à la représentation des phrases d'un point de vue statistique.\n",
    "\n",
    "Il est difficile pour un algorithme de travailler avec du texte, mais si nous pouvons convertir ce texte en une suite de chiffres alors nous pourrons en tirer certaines informations qui permettront à l'algorithme de travailler.\n",
    "\n",
    "Nous étudierons deux techniques de modélisation du texte sous forme de vecteur:\n",
    "* Bag of Words (BoW)\n",
    "* Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "### Bag of Words (BoW)\n",
    "\n",
    "Bag of Words ou Sac de Mots est une technique de modélisation du texte sous forme de vecteur où chaque élément représente la fréquence d'un token (ici mots) dans le corpus.\n",
    "\n",
    "Par exemple:\n",
    "\n",
    "```\n",
    "John aime regarder des films, Mary aime les films aussi.\n",
    "```\n",
    "\n",
    "Deviendra, après tokenisation et regroupement des mots:\n",
    "\n",
    "```\n",
    "\"John\", \"aime\", \"regarder\", \"des\", \"films\", \"Mary\", \"les\", \"aussi\"\n",
    "```\n",
    "\n",
    "On obtiendra alors la répresentation vectorielle suivante:\n",
    "\n",
    "```\n",
    "[1, 2, 1, 1, 2, 1, 1, 1]\n",
    "```\n",
    "\n",
    "*Ce vecteur ne préserve pas l'ordre des mots.*\n",
    "\n",
    "#### Implémentation en Python:\n",
    "\n",
    "Il est possible de le programmer soi-même ou d'utiliser une librairie Python tel que scikit-learn qui fera aussi le pré-processing:\n",
    "\n",
    "``` python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sentence = \"John aime regarder des films, Mary aime les films aussi.\"\n",
    "\n",
    "CountVec = CountVectorizer(ngram_range=(1,1), stop_words=stopwords.words('french'))\n",
    "\n",
    "Count_data = CountVec.fit_transform([sentence])\n",
    "\n",
    "print(CountVec.get_feature_names_out())\n",
    "print(Count_data.toarray())\n",
    "```\n",
    "\n",
    "On obtient alors:\n",
    "\n",
    "```\n",
    "['aime' 'aussi' 'films' 'john' 'mary' 'regarder']\n",
    "[[2 1 2 1 1 1]]\n",
    "```\n",
    "\n",
    "Cependant nous pouvons nous poser la question de la pertinence de cette information: un mot fréquent est il porteur de sens ?\n",
    "\n",
    "La réponse est non, si un mot survient plusieurs fois dans un document mais aussi dans un nombre important de documents alors, peut être, que ce mot est juste un mot fréquent comme une conjonction de coordination, sans sens particulier donc.\n",
    "\n",
    "### Term Frequency-Inverse Document Frenquency (TF-IDF)\n",
    "\n",
    "Afin de remédier au problème du Sac de Mots une approche serait de redéfinir la fréquence des mots afin de constituer un indice qui permette de connaître l'importance d'un mot dans un corpus de textes, ainsi les mots présent dans de nombreux documents seront pénalisés.\n",
    "\n",
    "Pour cela on utilise deux indices que l'on multipliera:\n",
    "1. Term Frequency\n",
    "2. Inverse Document Frequency\n",
    "\n",
    "Tout d'abord le Term Frequency ou la fréquence des termes se calcule de la manière suivante:\n",
    "\n",
    "$$TF = \\frac{n}{N} \\qquad \\text{Où:} \\begin{equation} \\begin{cases} n = \\text{Nombre d'instance du mot dans le document} \\\\ N = \\text{Nombre total de mots dans le document} \\end{cases} \\end{equation}$$\n",
    "\n",
    "Ensuite nous avons l'Inverse Document Frequency ou fréquence inverse du document qui est calculée en prenant le nombre total de documents du dataset, le divisant par le nombre de document contenant le mot puis en appliquant un logarithme:\n",
    "\n",
    "$$IDF = 1+\\log(\\frac{D}{Dn}) \\qquad \\text{Où:} \\begin{equation} \\begin{cases} D = \\text{Nombre total de documents} \\\\ Dn = \\text{Nombre total de documents contenant le mot} \\end{cases} \\end{equation}$$\n",
    "\n",
    "Enfin nous multiplions ces deux termes pour obtenir le Term frequency-Inverse Document Frequency (TF-IDF):\n",
    "\n",
    "$$TF-IDF = TF*IDF \\Rightarrow TF-IDF = \\frac{n}{N}+\\frac{n}{N}log(\\frac{D}{Dn})$$\n",
    "\n",
    "#### Implémentation en Python:\n",
    "\n",
    "``` python\n",
    "import re\n",
    "import nltk\n",
    "import unidecode\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "def text_processing(text):\n",
    "    ''' Return cleaned text for Machine Learning '''\n",
    "    REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "    NEW_LINE = re.compile('\\n')\n",
    "    BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "    STEMMER = SnowballStemmer('french')\n",
    "\n",
    "    text = text.lower()\n",
    "    text = unidecode.unidecode(text)\n",
    "    text = NEW_LINE.sub(' ',text)\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ',text)\n",
    "    text = BAD_SYMBOLS_RE.sub(' ',text)\n",
    "    text = ' '.join([STEMMER.stem(word) for word in word_tokenize(text)])\n",
    "    return text\n",
    "\n",
    "sentence1 = \"John aime regarder des films, Mary aime les films aussi.\"\n",
    "sentence2 = \"Bertrand aime les pizzas, mais les pizzas ne l'aiment pas en retour.\"\n",
    "sentence3 = \"Le muay thai est supérieur au kick boxing qui est lui même supérieur à la boxe anglaise\"\n",
    "\n",
    "tf_idf_vec = TfidfVectorizer(use_idf=True, ngram_range=(1,1), stop_words=stopwords.words('french'))\n",
    "\n",
    "tf_idf_data = tf_idf_vec.fit_transform([text_processing(sentence1),text_processing(sentence2), text_processing(sentence3)])\n",
    "\n",
    "print(tf_idf_vec.get_feature_names_out())\n",
    "print(tf_idf_data.toarray())\n",
    "```\n",
    "\n",
    "on obtient alors:\n",
    "\n",
    "```\n",
    "['aim' 'aiment' 'anglais' 'auss' 'bertrand' 'box' 'boxing' 'film' 'john'\n",
    " 'kick' 'mary' 'mem' 'muay' 'pizz' 'regard' 'retour' 'superieur' 'thai']\n",
    "[[0.4736296  0.         0.         0.311383   0.         0.\n",
    "  0.         0.62276601 0.311383   0.         0.311383   0.\n",
    "  0.         0.         0.311383   0.         0.         0.        ]\n",
    " [0.27626457 0.36325471 0.         0.         0.36325471 0.\n",
    "  0.         0.         0.         0.         0.         0.\n",
    "  0.         0.72650942 0.         0.36325471 0.         0.        ]\n",
    " [0.         0.         0.30151134 0.         0.         0.30151134\n",
    "  0.30151134 0.         0.         0.30151134 0.         0.30151134\n",
    "  0.30151134 0.         0.         0.         0.60302269 0.30151134]]\n",
    "```\n",
    "\n",
    "*Malheureusement le Stemmer a commis quelques erreurs par exemple 'aim' et 'aiment' ou 'box' et 'boxing' qui pourtant devraient avoir la même racine.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286dc737",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
