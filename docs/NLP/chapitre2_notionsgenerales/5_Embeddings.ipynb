{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abcaa4e5",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "## Sommaire\n",
    "* [Qu'est ce qu'un embedding ?](#qu-est-ce-qu-un-embedding)\n",
    "* [Mesure de la similarité](#mesure-de-la-similarite)\n",
    "* [Word2Vec](#word2vec)\n",
    "  * [Modèle: skipgram](#modele-skipgram)\n",
    "  * [Modèle: sac de mots continus](#modele-sac-de-mots-continus)\n",
    "  * [Mise en pratique](#id1)\n",
    "* [Glove: Global Vectors](#glove-global-vectors)\n",
    "  * [Co-occurence de mots](#co-occurence-de-mots)\n",
    "  * [Association avec des vecteurs](#association-avec-des-vecteurs)\n",
    "  * [Mise en pratique](#mise-en-pratique)\n",
    "* [Exploration de topics: t-Distributed Stochastic Neighbor Embedding (t-SNE)](#exploration-de-topics-t-distributed-stochastic-neighbor-embedding-t-sne)\n",
    "  * [Paranthèse sur le Gaussian](#paranthese-sur-le-gaussian)\n",
    "  * [Divergence de Kullback-Leibler](#divergence-de-kullback-leibler)\n",
    "  * [Mise en pratique](#id2)\n",
    "\n",
    "## Qu'est ce qu'un embedding ?\n",
    "\n",
    "Un embedding est une représentation mathématique d'un objet ou d'un concept sous la forme d'un vecteur le permettant ainsi d'être évalué par un programme informatique, et plus particulièrement un réseau de neurones.\n",
    "\n",
    "Pour illustrer ce qu'est un embedding nous prendrons pour exemple: une voiture, une moto et un être humain:\n",
    "\n",
    "* Une voiture possède 4 roues et est un véhicule, nous lui attribuons donc le vecteur: [1, 1, 1, 1, 1]\n",
    "* Une moto possède 2 roues et est un véhicule, nous lui attribuons le vecteur: [1, 0, 0, 1, 1]\n",
    "* Enfin un humain ne possède pas de roue et n'est pas un véhicule, donc: [0, 0, 0, 0, 0]\n",
    "\n",
    "<p align=\"center\"> <b>Exemple d'embedding simpliste</b>\n",
    "<img src=\"https://user-images.githubusercontent.com/65224852/186540909-8b3257a5-4e3c-499d-a556-1f4308e43196.png\">\n",
    "</p>\n",
    "\n",
    "C'est une manière de représenter les données cependant il en existe d'autres comme le sac de mots, par rapport aux phrases, que l'on a étudié dans le chapitre sur la modélisation statistique du langage.\n",
    "\n",
    "Ainsi la machine peut comparer les données et les interpréter, pour comparer deux embeddings nous utilisons une mesure de similarité.\n",
    "\n",
    "Il faut cependant avoir à l'esprit que les meilleurs algorithmes d'embedding ne fournissent pas des caractéristiques bien précises à analyser tel que le nombre de roues ou le sexe d'une personne, au lieu de cela c'est une combinaison de facteurs qui est utilisé et leur étude permet notamment de combattre les biais en machine learning dans le cadre du NLP.\n",
    "\n",
    "## Mesure de la similarité\n",
    "\n",
    "La mesure de similarité la plus courante pour comparer deux vecteurs est la similarité cosinus (ou cosine similarity).\n",
    "\n",
    "Pour obtenir la similarité cosinus on part d'un produit vectoriel:\n",
    "\n",
    "$$\\begin{align} &A \\cdot B = \\left\\| A \\right\\| \\left\\| B \\right\\| cos(\\theta)\n",
    "\\\\ \\implies &cos(\\theta) = \\frac{A \\cdot B}{\\left\\| A \\right\\| \\left\\| B \\right\\|}\n",
    "\\\\ \\implies &cos(\\theta) = \\frac{\\sum_{i=1}^nA_iB_i}{\\sqrt{\\sum_{i=1}^nA_i^2}\\sqrt{\\sum_{i=1}^nB_i^2}}\n",
    "\\end{align}$$\n",
    "\n",
    "Suivant la valeur de $cos(\\theta)$ on étudie la similarité des vecteurs A et B:\n",
    "* Lorsque $cos(\\theta)$ tend vers 1 les deux vecteurs sont colinéaires de facteur positif, en d'autres termes similaires.\n",
    "* Lorsque $cos(\\theta)$ tend vers 0 les deux vecteurs sont orthogonaux, soit non similaire\n",
    "* Lorsque $cos(\\theta)$ tend vers -1 les deux vecteurs sont colinéaires de facteur négatif, soit opposés.\n",
    "\n",
    "Maintenant utilisons cette mesure de similarité sur notre exemple précédent pour étudier la proximité des vecteurs:\n",
    "\n",
    "**Comparaison Voiture et Moto:**\n",
    "* $V \\cdot M = 1*1 + 1*0 + 1*0 + 1*1 + 1*1 = 3$\n",
    "* $\\left\\| V \\right\\| \\left\\| M \\right\\| = \\sqrt{1^2 + 1^2 + 1^2 + 1^2 + 1^2} \\sqrt{1^2 + 0^2 + 0^2 + 1^2 + 1^2} = \\sqrt{5} \\sqrt{3} = \\sqrt{15}$\n",
    "\n",
    "$\\implies cos(\\theta) = \\frac{V \\cdot M}{\\left\\| V \\right\\| \\left\\| M \\right\\|} = \\frac{3}{\\sqrt{15}} \\simeq 0,7746$\n",
    "  \n",
    "**Comparaison Voiture et Humain:**\n",
    "* $V \\cdot M = 1*0 + 1*0 + 1*0 + 1*0 + 1*0 = 0$\n",
    "\n",
    "$\\implies cos(\\theta) = 0$\n",
    "  \n",
    "Sans surprise la moto a plus de similarité avec la voiture que l'humain.\n",
    "\n",
    "## Word2Vec\n",
    "\n",
    "<p align=\"center\"> <i>Pour cette partie il est fortement recommandé d'avoir étudié les réseaux de neurones et les calculs sous-jacents tel que la backpropagation.\n",
    "\n",
    "Source de cette partie: https://arxiv.org/pdf/1411.2738.pdf.</i></p>\n",
    "\n",
    "Word2Vec est une famille d'algorithmes ayant pour but de fournir des embeddings de grande qualité, nous étudierons ici deux algorithmes: le modèle skipgram et le modèle sac de mots continus (ou continuous bag of words).\n",
    "\n",
    "### Modèle: skipgram\n",
    "\n",
    "L'algorithme skipgram de Word2Vec prend en entrée un mot et à partir de ce mot reconstitue un contexte composé de T mots (T étant la taille du contexte visé), pour cela le modèle utilise un réseau de neurone feed-forward à une seule couche avec un **Hierarchical Softmax** afin d'obtenir une représentation vectorielle optimisée *(aussi nommé embedding)* du mot $w_n$.\n",
    "\n",
    "<p align=\"center\"> <b>Architecture Word2Vec: skipgram</b>\n",
    "<img src=\"https://user-images.githubusercontent.com/65224852/186547778-0844025a-9ebe-49c1-a43f-7539dc2228bf.png\">\n",
    "</p>\n",
    "\n",
    "L'objectif de l'entraînement est d'optimiser cette fonction:\n",
    "\n",
    "$$\\begin{align}E &= -log \\, p(w_{n-T}, \\cdots, w_{n+T}|w_n)\n",
    "\\\\ &= -log\\prod_{t=-T}^T p(w_{n+t}|w_n)\\end{align}$$\n",
    "\n",
    "Où:\n",
    "* $w_n$: Mot d'indice n.\n",
    "* $M$: Cardinal de l'ensemble des mots.\n",
    "\n",
    "Le softmax habituel est trop coûteux à calculer en raison du trop grand nombre de mots, la formule serait la suivante:\n",
    "\n",
    "$$p(w_{n+t}|w_n) = \\frac{v'_{w_{n+t}} \\,^\\top v_{w_n}}{\\sum_{i=1}^M \\exp(v'_{w_i} \\,^\\top v_{w_n})}$$\n",
    "\n",
    "Où:\n",
    "* $t$: Contexte de taille T (*voir comme un intervalle*) pour un mot en position n.\n",
    "* $v_{w_n}$: vecteur pris en entrée du mot d'indice n.\n",
    "* $v'_{w_n}$: vecteur de sortie du mot d'indice n.\n",
    "\n",
    "#### Hierarchical Softmax:\n",
    "\n",
    "Le Hierarchical Softmax est une méthode plus efficace pour calculer le softmax grâce à sa complexité $O(log(M))$ tandis que le softmax a une complexité $O(M)$.\n",
    "\n",
    "Pour cela on considère le réseau de neurones comme un **arbre binaire de Huffman** *(possède strictement deux enfants par noeud)* où chaque feuille représente un mot du vocabulaire, il existe un unique chemin de la racine au mot et ce chemin est utilisé pour calculer la probabilité des mots.\n",
    "\n",
    "<p align=\"center\"> <b>Hierarchical Softmax: Arbre binaire de Huffman</b>\n",
    "<img src=\"https://user-images.githubusercontent.com/65224852/186721250-a73309b6-1ec8-4910-a557-29c6429ad4b3.png\" width=600 height=300>\n",
    "</p>\n",
    "\n",
    "Où:\n",
    "* $n(w, j)$: j-éme noeud du chemin au mot $w$.\n",
    "* $L(w)$: Longueur du chemin au mot $w$.\n",
    "\n",
    "Dans ce modèle il n'y a pas de vecteur de sortie au niveau des feuilles mais chaque noeud correspond à un vecteur, *(il y en a en tout $M-1$)*, le but est d'optimiser le vecteur de la racine qui correspond au mot $w_n$.\n",
    "\n",
    "La probabilité d'obtenir un mot est la suivante:\n",
    "\n",
    "$$p(w | w_n) = \\prod_{j=1}^{L(w_n)-1} \\sigma([n(w,j+1) = ch(n(w,j))] \\cdot v'_{n(w,j)} \\,^\\top v_{w_n})$$\n",
    "\n",
    "Où:\n",
    "* $ch(n)$: est l'enfant de gauche du noeud n.\n",
    "* $v'_{n(w,j)}$: représentation vectorielle du noeud j.\n",
    "* $[n(w,j+1) = ch(n(w,j))]$: vaut 1 si vrai sinon -1.\n",
    "* $\\sigma(x) = \\frac{1}{1+e^{-x}}$: fonction d'activation sigmoid.\n",
    "\n",
    "Pour comprendre cette formule nous utiliserons un exemple où l'on calcule la probabilité d'obtenir $w_2$ d'après le schéma précédent, il faut voir le calcul de cette probabilité comme une marche aléatoire de noeud en noeud.\n",
    "\n",
    "Pour chaque noeud *(incluant la racine)* nous définissons la probabilité d'aller à droite ou à gauche pour un noeud $j$ de la manière suivante:\n",
    "\n",
    "$$\\begin{equation} \\begin{cases} p(j, left) = \\sigma(v'_j \\cdot \\,^\\top v_{w_n})\n",
    "\\\\ p(j, right) = 1 - \\sigma(v'_j \\cdot \\,^\\top v_{w_n}) = \\sigma(-v'_j \\cdot \\,^\\top v_{w_n}) \\end{cases} \\end{equation}$$\n",
    "\n",
    "Ainsi la probabilité d'obtenir $w_2$ est:\n",
    "\n",
    "$$\\begin{align} p(w_2 | w_n) &= p(n(w_2, 1), left) \\cdot p(n(w_2, 2), left) \\cdot p(n(w_2, 3), right)\n",
    "\\\\ &= \\sigma(v'_{n(w_2, 1)} \\,^\\top v_{w_n}) \\cdot \\sigma(v'_{n(w_2, 2)} \\,^\\top v_{w_n}) \\cdot \\sigma(v'_{n(w_2, 3)} \\,^\\top v_{w_n}) \\end{align}$$\n",
    "\n",
    "Et cette marche aléatoire défini une distribution multinomiale parmis tous les mots du vocabulaire.\n",
    "\n",
    "### Modèle: sac de mots continus\n",
    "\n",
    "Ce modèle a un fonctionnement opposé au modèle skipgram, il prend en entrée un contexte de $T$ mots et donne la probabilité d'obtenir un mot spécifique pour ce contexte, pour cela le modèle utilise un réseau de neurone feed-forward à une seule couche afin d'obtenir une représentation vectorielle optimisée des mots du contexte $(w_{n-T}, \\cdots, w_{n+T})$.\n",
    "\n",
    "<p align=\"center\"> <b>Architecture Word2Vec: sac de mots continus</b>\n",
    "<img src=\"https://user-images.githubusercontent.com/65224852/186798004-27c63a1a-c868-4088-bc92-1c091db86dbb.png\">\n",
    "</p>\n",
    "\n",
    "L'objectif de l'entraînement est d'optimiser cette fonction:\n",
    "\n",
    "$$\\begin{align}E &= -log \\, p(w_n | w_{n-T}, \\cdots, w_{n+T})\n",
    "\\\\ &=-v'_{w_n} \\cdot \\,^\\top h + log \\sum_{j=1}^M exp(v'_{w_j} \\cdot \\,^\\top h)\\end{align}$$\n",
    "\n",
    "Où:\n",
    "* $v'_{w_n}$: représentation vectorielle de sortie du mot n.\n",
    "* $h$: moyenne des vecteurs pris en entrée:\n",
    "\n",
    "$$h = \\frac{1}{T}(v_{w_{n-T}} + v_{w_{n-T+1}} + \\cdots + v_{w_{n+T}})$$\n",
    "\n",
    "### Mise en pratique:\n",
    "\n",
    "Dans cette partie nous essayerons de comparer les représentations vectorielles de différents mots à l'aide de Word2Vec et de la similarité cosinus.\n",
    "\n",
    "Pour cela nous utiliserons la librairie Gensim et ntlk, pour installer les requirements tapez:\n",
    "\n",
    "```\n",
    "pip install nltk\n",
    "pip install gensim\n",
    "pip install unidecode\n",
    "```\n",
    "\n",
    "Le dataset utilisé est un petit corpus de textes présent dans la page github, il n'est pas suffisament grand pour avoir de bonnes corrélations mais une petite idée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "366047a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Modèle CBOW ===\n",
      "Similarité cosinus entre 'jour' et 'stylo' - CBOW :  0.85024124\n",
      "Top 10 mots corrélés à 'jour' - CBOW :  [('encore', 0.9996173977851868), ('alors', 0.9996070861816406), ('fut', 0.9996036291122437), (\"qu'on\", 0.9995915293693542), ('oui', 0.9995855689048767)]\n",
      "\n",
      "=== Modèle Skip Gram ===\n",
      "Similarité cosinus entre 'jour' et 'stylo' - Skip Gram :  0.8073238\n",
      "Top 10 mots corrélés à 'jour' - Skip Gram :  [('lendemain', 0.9945391416549683), ('lire', 0.9927944540977478), ('ecrit', 0.9927003383636475), ('point', 0.9919275045394897), ('francais', 0.9918561577796936)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "import warnings\n",
    "import unidecode\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize\n",
    " \n",
    "def text_processing(text):\n",
    "    ''' Return cleaned text for Machine Learning '''\n",
    "    REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "    NEW_LINE = re.compile('\\n')\n",
    "    BAD_SYMBOLS_RE = re.compile('[^a-z\\' #+_]')\n",
    "\n",
    "    text = text.lower()\n",
    "    text = unidecode.unidecode(text)\n",
    "    text = NEW_LINE.sub(' ',text)\n",
    "    text = REPLACE_BY_SPACE_RE.sub('',text)\n",
    "    text = BAD_SYMBOLS_RE.sub(' ',text)\n",
    "    return text\n",
    "    \n",
    "#nltk.download('punkt')\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "    \n",
    "total_text = \"\"\n",
    "data = []\n",
    "\n",
    "#  Reads files\n",
    "train_files = ['data/corpus_1.txt', 'data/corpus_2.txt', 'data/corpus_3.txt', 'data/corpus_4.txt']\n",
    "\n",
    "for file_path in train_files:\n",
    "    with open(file_path,'r', encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            tmp = line\n",
    "            tmp = tmp.replace(\"\\n\", \" \")\n",
    "            total_text = total_text + tmp\n",
    "                \n",
    "for s in sent_tokenize(total_text):\n",
    "    temp = []\n",
    "\n",
    "    # tokenize the sentence into data\n",
    "    content = text_processing(s)\n",
    "    token = content.split()\n",
    "    data.append(token)\n",
    "    \n",
    "# Create CBOW model\n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100, window = 5)\n",
    " \n",
    "# Print results\n",
    "print(\"=== Modèle CBOW ===\")\n",
    "print(\"Similarité cosinus entre 'jour' \" + \"et 'stylo' - CBOW : \", model1.wv.similarity('jour', 'stylo'))\n",
    "print(\"Top 10 mots corrélés à 'jour' - CBOW : \", model1.wv.most_similar('jour', topn=5))\n",
    " \n",
    "# Create Skip Gram model\n",
    "model2 = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100, window = 5, sg = 1)\n",
    " \n",
    "# Print results\n",
    "print(\"\\n=== Modèle Skip Gram ===\")\n",
    "print(\"Similarité cosinus entre 'jour' \" + \"et 'stylo' - Skip Gram : \", model2.wv.similarity('jour', 'stylo'))\n",
    "print(\"Top 10 mots corrélés à 'jour' - Skip Gram : \", model2.wv.most_similar('jour', topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580a0b6c",
   "metadata": {},
   "source": [
    "Dans cette situation skip gram a de meilleures performances que CBOW.\n",
    "\n",
    "Skip-gram fonctionne bien avec de petites quantités de données d'entraînement, et représente bien les mots rares ou les phrases.\n",
    "\n",
    "Tandis que CBOW est beaucoup plus rapide à entraîner que le modèle skip-gram, et possède une précision légérement meilleure pour les mots fréquents.\n",
    "\n",
    "## GloVe: Global Vectors\n",
    "\n",
    "<p align=\"center\"> <i>Source de cette partie: <a href=\"https://aclanthology.org/D14-1162.pdf\">https://aclanthology.org/D14-1162.pdf</a></i></p>\n",
    "\n",
    "A l'instar de Word2Vec, GloVe vise à fournir des embeddings de grande qualité à partir de la distribution de probabilité des mots dans un corpus donné.\n",
    "\n",
    "GloVe signifie *Global Vectors* puisque dans ce modèle les statistiques globales du corpus sont capturées directement par le modèle.\n",
    "\n",
    "La question est de savoir comment à partir de ces statistiques on peut obtenir du sens pour les mots sous forme de vecteur.\n",
    "\n",
    "### Co-occurence de mots:\n",
    "\n",
    "Tout débute avec les probabilités de co-occurence de mots, notons:\n",
    "\n",
    "* $X$: matrice de co-occurence de mots.\n",
    "* $X_{i,j}$: nombre de fois qu'un mot $j$ apparaît dans le contexte d'un mot $i$.\n",
    "* $X_i = \\sum_k^M X_{i,k}$ le nombre de fois qu'un mot quelconque $k$ apparaît dans le contexte du mot $i$.\n",
    "* $P_{i,j} = P(j|i) = \\frac{X_{i,j}}{X_i}$ la probabilité qu'un mot $j$ apparaît dans contexte d'un mot $i$.\n",
    "\n",
    "D'après ces notations et une série d'exemples basées sur des phases en thermodynamique *(glace, vapeur, solide...)* on observe le tableau suivant:\n",
    "\n",
    "<p align=\"center\"> <b>Tableau de probabilités de co-occurence</b>\n",
    "<img src=\"https://user-images.githubusercontent.com/65224852/188535001-079ccb85-4bc7-463c-895b-4910e979c217.png\">\n",
    "</p>\n",
    "\n",
    "Comme on pourrait s'y attendre: le mot **glace** apparaît plus fréquemment avec le mot **solide** que le mot **gas**, tandis que le mot **vapeur** apparaît plus fréquemment avec le mot **gas** qu'avec le mot **solide**, les deux mots apparaissent à part égale avec le mot **eau** et enfin le mot **fashion** n'apparaît presque jamais.\n",
    "\n",
    "Le ratio de probabilités de co-occurence est capable de distinguer des mots corrélés par rapport à des mots non corrélés.\n",
    "\n",
    "Ainsi un bon point de départ pour l'apprentissage de représentations vectorielles de mots devrait être le ratio des probabilités de co-occurence plutôt que les probabilités elle-même.\n",
    "\n",
    "### Association avec des vecteurs:\n",
    "\n",
    "Soit le modèle général:\n",
    "\n",
    "$$F(w_i, w_j, \\tilde{w}_k) = \\frac{P_{i,k}}{P_{j,k}}$$\n",
    "\n",
    "Où:\n",
    "* $w_i$: vecteur du mot $i$.\n",
    "* $\\tilde{w}_i$: vecteur d'un mot $i$ d'un contexte séparé.\n",
    "\n",
    "On souhaite encoder l'information présent dans le ratio $\\frac{P_{i,k}}{P_{j,k}}$ et puisque les espaces vectoriels sont des structures linéaires la manière la plus naturelle de le faire est de prendre la différence des vecteurs:\n",
    "\n",
    "$$F(w_i - w_j, \\tilde{w}_k) = \\frac{P_{i,k}}{P_{j,k}}$$\n",
    "\n",
    "Puis on pose:\n",
    "\n",
    "$$F((w_i - w_j) \\,^\\top \\tilde{w}_k) = \\frac{P_{i,k}}{P_{j,k}}$$\n",
    "\n",
    "La distinction entre un mot et un mot du contexte est arbitraire, nous sommes libre d'échanger les deux rôles. Ainsi $w \\leftrightarrow \\tilde{w}$ et $X \\leftrightarrow X \\,^\\top$, le modèle doit donc être invariant à cette modification mais ce n'est pas le cas avec la formule précédente.\n",
    "\n",
    "On résout le problème en posant:\n",
    "\n",
    "$$\\begin{align} F((w_i - w_j) \\,^\\top \\tilde{w}_k) &= \\frac{F(w_i \\,^\\top \\tilde{w}_k)}{F(w_j \\,^\\top \\tilde{w}_k)} = \\frac{P_{i,k}}{P_{j,k}}\n",
    "\\\\ \\implies F(w_i \\,^\\top \\tilde{w}_k) &= P_{i,k} = \\frac{X_{i,k}}{X_i} \\end{align}\n",
    "\\\\ \\implies F = exp \\quad \\text{ et } \\quad w_i \\,^\\top \\tilde{w}_k = log(P_{i,k}) = log(X_{i,k}) - log(X_i)$$\n",
    "\n",
    "Enfin pour que cette équation possède une symétrie par l'échange des contextes il faudrait modifier $log(X_i)$, ce terme est indépendant de $k$ donc il peut être absorbé dans un biais $b_i$ pour $w_i$, puis nous ajoutons un biais $\\tilde{b}_k$ pour $\\tilde{w}_k$, restorant la symétrie.\n",
    "\n",
    "$$\\begin{align} \\implies w_i \\,^\\top \\tilde{w}_k + log(X_i) &= log(X_{i,k})\n",
    "\\\\ \\implies w_i \\,^\\top \\tilde{w}_k + b_i + \\tilde{b}_k &= log(X_{i,k}) \\end{align}$$\n",
    "\n",
    "C'est une simplification drastique de la toute première équation *(le modèle général)* et mal définie puisque le logarithme diverge en 0.\n",
    "\n",
    "C'est pourquoi GloVe utilise un modèle de régression des moindres carrés pondérés, remplaçant l'équation précédente en un problème des moindres carrés pondéré par une fonction $f$ tel que:\n",
    "\n",
    "$$J = \\sum_{i,j=1}^V f(X_{i,j}) \\, (w_i \\,^\\top \\tilde{w}_k + b_i + \\tilde{b}_j - log(X_{i,j}))^2$$\n",
    "\n",
    "Où:\n",
    "* $V$: taille du vocabulaire.\n",
    "\n",
    "La fonction $f$ doit obéir à certaines régles tel que:\n",
    "* $f(0) = 0$ rendant $lim_{x \\rightarrow 0} \\, f(x) \\, log^2(x)$ fini.\n",
    "* $f(x)$ ne doit pas être décroissante.\n",
    "* $f(x)$ doit être relativement petite pour de grandes valeurs de x.\n",
    "\n",
    "Un grand nombre de fonctions satisfont ces propriétés mais une classe de fonctions fonctionne plutôt bien et est paramétrisée de la sorte:\n",
    "\n",
    "$$\\begin{equation} f(x) = \\begin{cases} (x/x_{\\text{max}})^\\alpha \\quad &\\text{si } x < x_{\\text{max}}\n",
    "\\\\ 1 \\quad &\\text{sinon} \\end{cases} \\end{equation}$$\n",
    "\n",
    "De manière empirique les paramétres retenus sont $\\alpha = 3/4$ et $x_{\\text{max}} = 100$.\n",
    "\n",
    "### Mise en pratique:\n",
    "\n",
    "Pour cette mise en pratique nous recoderons entièrement l'algorithme étant donné que les packages en ligne ne fonctionnent pas.\n",
    "\n",
    "Habituellement on télécharge un fichier texte contenant les vecteurs d'une version pré-entraînée de GloVe.\n",
    "\n",
    "*Aucune version n'est disponible sur pip et cloner le repertoire de l'université de Stanford: https://github.com/stanfordnlp/GloVe ne permet pas d'être compilé sous Windows à cause de <pthread.h>.*\n",
    "\n",
    "Pour mener à bien notre implémentation de GloVe il faudra installer:\n",
    "\n",
    "```\n",
    "pip install gensim\n",
    "pip install torch\n",
    "pip install dataclasses\n",
    "pip install h5py\n",
    "pip install tqdm\n",
    "pip install numpy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c27b4086",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=== Etape 0- Données d'entraînement ===#\n",
    "\n",
    "import gensim.downloader as api\n",
    "import itertools\n",
    "\n",
    "dataset = api.load(\"text8\")\n",
    "corpus = list(itertools.chain.from_iterable(dataset))\n",
    "corpus = corpus[0:100000]\n",
    "\n",
    "#Nous entraînerons l'algorithme sur les 100000 premier mots\n",
    "#étant donné que le temps d'apprentissage peut être très long !\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f5adc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#=== Paramétres ===#\n",
    "\n",
    "vocab_size = 10000 #100000 normalement\n",
    "window_size = 10\n",
    "num_partitions = 10\n",
    "chunk_size = 1000000\n",
    "cooccurrence_dir = os.getcwd()\n",
    "output_filepath = os.getcwd()+\"glove.pth\"\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "device = \"cpu\"\n",
    "learning_rate = 0.05\n",
    "embedding_size = 100\n",
    "x_max = 100\n",
    "alpha = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adc84472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#=== Etape 1- Compter les paires co-occurente ===#\n",
    "\n",
    "\"\"\"Il faut déterminer le vocabulaire, c'est un ensemble de tokens associés\n",
    "à un entier, si un token n'appartient pas au corpus il est représenté par 'unk'.\n",
    "Nous n'utiliserons qu'un subset de tokens: le top k des tokens les plus fréquents\"\"\"\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class Vocabulary:\n",
    "    token2index: dict = field(default_factory=dict) #Dictionnaire qui associe un token à un entier.\n",
    "    index2token: dict = field(default_factory=dict) #Dictionnaire qui associe un entier à un token.\n",
    "    token_counts: list = field(default_factory=list) #Liste où la i-éme valeur est le nombe de token avec l'index i.\n",
    "    _unk_token: int = field(init=False, default=-1) #Entier pour les tokens inconnus.\n",
    "    \n",
    "    def add(self, token):\n",
    "        #Ajoute un token au vocabulaire.\n",
    "        if token not in self.token2index:\n",
    "            index = len(self)\n",
    "            self.token2index[token] = index\n",
    "            self.index2token[index] = token\n",
    "            self.token_counts.append(0)\n",
    "        self.token_counts[self.token2index[token]] += 1\n",
    "    \n",
    "    def get_topk_subset(self, k):\n",
    "        #Créer un vocabulaire avec le top k des tokens.\n",
    "        tokens = sorted(\n",
    "            list(self.token2index.keys()),\n",
    "            key=lambda token: self.token_counts[self[token]],\n",
    "            reverse=True\n",
    "        )\n",
    "        return type(self)(\n",
    "            token2index={token: index for index, token in enumerate(tokens[:k])},\n",
    "            index2token={index: token for index, token in enumerate(tokens[:k])},\n",
    "            token_counts=[\n",
    "                self.token_counts[self.token2index[token]] for token in tokens[:k]\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def shuffle(self):\n",
    "        #Mélange aléatoirement les tokens et indices.\n",
    "        new_index = [_ for _ in range(len(self))]\n",
    "        random.shuffle(new_index)\n",
    "        new_token_counts = [None] * len(self)\n",
    "        for token, index in zip(list(self.token2index.keys()), new_index):\n",
    "            new_token_counts[index] = self.token_counts[self[token]]\n",
    "            self.token2index[token] = index\n",
    "            self.index2token[index] = token\n",
    "        self.token_counts = new_token_counts\n",
    "\n",
    "    def get_index(self, token):\n",
    "        return self[token]\n",
    "    \n",
    "    def get_token(self, index):\n",
    "        if not index in self.index2token:\n",
    "            raise Exception(\"Invalid index.\")\n",
    "        return self.index2token[index]\n",
    "    \n",
    "    @property\n",
    "    def unk_token(self):\n",
    "        return self._unk_token\n",
    "    \n",
    "    def __getitem__(self, token):\n",
    "        if token not in self.token2index:\n",
    "            return self._unk_token\n",
    "        return self.token2index[token]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.token2index)\n",
    "    \n",
    "@dataclass\n",
    "class Vectorizer:\n",
    "    vocab: Vocabulary\n",
    "\n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus, vocab_size):\n",
    "        \n",
    "        \"\"\"Créer un vocabulaire en ajoutant tous les tokens dans le corpus,\n",
    "        puis le top vocab_size est selectionné pour créer un nouveau vocabulaire,\n",
    "        ensuite on mélange.\"\"\"\n",
    "        \n",
    "        vocab = Vocabulary()\n",
    "        for token in corpus:\n",
    "            vocab.add(token)\n",
    "        vocab_subset = vocab.get_topk_subset(vocab_size)\n",
    "        vocab_subset.shuffle()\n",
    "        return cls(vocab_subset)\n",
    "\n",
    "    def vectorize(self, corpus):\n",
    "        return [self.vocab[token] for token in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "035fb18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=== Etape 1B- Scan des fenêtres de contexte ===#\n",
    "\n",
    "\"\"\"Scanner l'ensemble des données pour établir le nombre possible de paires\n",
    "co-occurente pourrait surcharger la RAM, c'est pourquoi on découpe en plusieurs\n",
    "scans, de plus pour préserver le compte entre chaque scan on utilisera une\n",
    "librairie h5py pour sauvegarder de grosse quantités de données.\"\"\"\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import h5py\n",
    "\n",
    "@dataclass\n",
    "class CooccurrenceEntries:\n",
    "    vectorized_corpus: list\n",
    "    vectorizer: Vectorizer\n",
    "    \n",
    "    @classmethod\n",
    "    def setup(cls, corpus, vectorizer):\n",
    "        return cls(\n",
    "            vectorized_corpus=vectorizer.vectorize(corpus),\n",
    "            vectorizer=vectorizer\n",
    "        )\n",
    "    \n",
    "    def validate_index(self, index, lower, upper):\n",
    "        is_unk = index == self.vectorizer.vocab.unk_token\n",
    "        if lower < 0:\n",
    "            return not is_unk\n",
    "        return not is_unk and index >= lower and index <= upper\n",
    "\n",
    "    def build(\n",
    "        self,\n",
    "        window_size,\n",
    "        num_partitions,\n",
    "        chunk_size,\n",
    "        output_directory=\".\"\n",
    "    ):\n",
    "        partition_step = len(self.vectorizer.vocab) // num_partitions\n",
    "        split_points = [0]\n",
    "        while split_points[-1] + partition_step <= len(self.vectorizer.vocab):\n",
    "            split_points.append(split_points[-1] + partition_step)\n",
    "        split_points[-1] = len(self.vectorizer.vocab)\n",
    "\n",
    "        for partition_id in tqdm(range(len(split_points) - 1)):\n",
    "            index_lower = split_points[partition_id]\n",
    "            index_upper = split_points[partition_id + 1] - 1\n",
    "            cooccurr_counts = Counter()\n",
    "            for i in tqdm(range(len(self.vectorized_corpus))):\n",
    "                if not self.validate_index(\n",
    "                    self.vectorized_corpus[i],\n",
    "                    index_lower,\n",
    "                    index_upper\n",
    "                ):\n",
    "                    continue\n",
    "                \n",
    "                context_lower = max(i - window_size, 0)\n",
    "                context_upper = min(i + window_size + 1, len(self.vectorized_corpus))\n",
    "                for j in range(context_lower, context_upper):\n",
    "                    if i == j or not self.validate_index(\n",
    "                        self.vectorized_corpus[j],\n",
    "                        -1,\n",
    "                        -1\n",
    "                    ):\n",
    "                        continue\n",
    "                    cooccurr_counts[(self.vectorized_corpus[i], self.vectorized_corpus[j])] += 1 / abs(i - j)\n",
    "\n",
    "            cooccurr_dataset = np.zeros((len(cooccurr_counts), 3))\n",
    "            for index, ((i, j), cooccurr_count) in enumerate(cooccurr_counts.items()):\n",
    "                cooccurr_dataset[index] = (i, j, cooccurr_count)\n",
    "            if partition_id == 0:\n",
    "                file = h5py.File(\n",
    "                    os.path.join(\n",
    "                        output_directory,\n",
    "                        \"cooccurrence.hdf5\"\n",
    "                    ),\n",
    "                    \"w\"\n",
    "                )\n",
    "                dataset = file.create_dataset(\n",
    "                    \"cooccurrence\",\n",
    "                    (len(cooccurr_counts), 3),\n",
    "                    maxshape=(None, 3),\n",
    "                    chunks=(chunk_size, 3)\n",
    "                )\n",
    "                prev_len = 0\n",
    "            else:\n",
    "                prev_len = dataset.len()\n",
    "                dataset.resize(dataset.len() + len(cooccurr_counts), axis=0)\n",
    "            dataset[prev_len: dataset.len()] = cooccurr_dataset\n",
    "        \n",
    "        file.close()\n",
    "        with open(os.path.join(output_directory, \"vocab.pkl\"), \"wb\") as file:\n",
    "            pickle.dump(self.vectorizer.vocab, file)\n",
    "            \n",
    "@dataclass\n",
    "class CooccurrenceDataset(torch.utils.data.Dataset):\n",
    "    token_ids: torch.Tensor\n",
    "    cooccurr_counts: torch.Tensor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return [self.token_ids[index], self.cooccurr_counts[index]]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.token_ids.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea038488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=== Etape 1C- Utilisation du code pour compter les paires ===#\n",
    "\n",
    "vectorizer = Vectorizer.from_corpus(\n",
    "    corpus=corpus,\n",
    "    vocab_size=vocab_size)\n",
    "\n",
    "cooccurrence = CooccurrenceEntries.setup(\n",
    "    corpus=corpus,\n",
    "    vectorizer=vectorizer)\n",
    "\n",
    "cooccurrence.build(\n",
    "    window_size=window_size,\n",
    "    num_partitions=num_partitions,\n",
    "    chunk_size=chunk_size,\n",
    "    output_directory=cooccurrence_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5d32f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=== Etape 2- Création du modèle ===#\n",
    "\n",
    "import torch\n",
    "import contextlib\n",
    "import torch.nn as nn\n",
    "\n",
    "@dataclass\n",
    "class HDF5DataLoader:\n",
    "    filepath: str\n",
    "    dataset_name: str\n",
    "    batch_size: int\n",
    "    device: str\n",
    "    dataset: h5py.Dataset = field(init=False)\n",
    "\n",
    "    def iter_batches(self):\n",
    "        chunks = list(self.dataset.iter_chunks())\n",
    "        random.shuffle(chunks)\n",
    "        for chunk in chunks:\n",
    "            chunked_dataset = self.dataset[chunk]\n",
    "            dataloader = torch.utils.data.DataLoader(\n",
    "                dataset=CooccurrenceDataset(\n",
    "                    token_ids=torch.from_numpy(chunked_dataset[:,:2]).long(),\n",
    "                    cooccurr_counts=torch.from_numpy(chunked_dataset[:,\n",
    "                        2]).float()\n",
    "                ),\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=True,\n",
    "                pin_memory=True\n",
    "            )\n",
    "            for batch in dataloader:\n",
    "                batch = [_.to(self.device) for _ in batch]\n",
    "                yield batch\n",
    "\n",
    "    @contextlib.contextmanager\n",
    "    def open(self):\n",
    "        with h5py.File(self.filepath, \"r\") as file:\n",
    "            self.dataset = file[self.dataset_name]\n",
    "            yield\n",
    "            \n",
    "class GloVe(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, x_max, alpha):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_size,\n",
    "            sparse=True\n",
    "        )\n",
    "        self.weight_tilde = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_size,\n",
    "            sparse=True\n",
    "        )\n",
    "        self.bias = nn.Parameter(\n",
    "            torch.randn(\n",
    "                vocab_size,\n",
    "                dtype=torch.float,\n",
    "            )\n",
    "        )\n",
    "        self.bias_tilde = nn.Parameter(\n",
    "            torch.randn(\n",
    "                vocab_size,\n",
    "                dtype=torch.float,\n",
    "            )\n",
    "        )\n",
    "        self.weighting_func = lambda x: (x / x_max).float_power(alpha).clamp(0, 1)\n",
    "    \n",
    "    def forward(self, i, j, x):\n",
    "        loss = torch.mul(self.weight(i), self.weight_tilde(j)).sum(dim=1)\n",
    "        loss = (loss + self.bias[i] + self.bias_tilde[j] - x.log()).square()\n",
    "        loss = torch.mul(self.weighting_func(x), loss).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4434614b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#=== Etape 2B- Entraînement du modèle ===#\n",
    "\n",
    "dataloader = HDF5DataLoader(\n",
    "    filepath=os.path.join(cooccurrence_dir, \"cooccurrence.hdf5\"),\n",
    "    dataset_name=\"cooccurrence\",\n",
    "    batch_size=batch_size,\n",
    "    device=device\n",
    ")\n",
    "model = GloVe(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=embedding_size,\n",
    "    x_max=x_max,\n",
    "    alpha=alpha\n",
    ")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adagrad(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate\n",
    ")\n",
    "with dataloader.open():\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        epoch_loss = 0\n",
    "        for batch in tqdm(dataloader.iter_batches()):\n",
    "            loss = model(\n",
    "                batch[0][:, 0],\n",
    "                batch[0][:, 1],\n",
    "                batch[1]\n",
    "            )\n",
    "            epoch_loss += loss.detach().item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        losses.append(epoch_loss)\n",
    "        print(f\"Epoch {epoch+1}: loss = {epoch_loss}\")\n",
    "        torch.save(model.state_dict(), output_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c753347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarité entre woman et man:\n",
      "0.028059274\n",
      "Mots les plus similaires de computer:\n",
      "['work', 'willers', 'exercising', 'fundamental', 'emphasizing', 'contradictions', 'stabbed', 'voyages', 'basic', 'beard']\n",
      "Mots les plus similaires de united:\n",
      "['chronologically', 'essentiallly', 'sinking', 'auguste', 'contested', 'lyman', 'conceal', 'nebraska', 'disprove', 'armoured']\n",
      "Mots les plus similaires de early:\n",
      "['confidence', 'produce', 'elected', 'theoretic', 'harmonious', 'gellius', 'ayers', 'climatology', 'analogy', 'templeton']\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "with open(os.path.join(cooccurrence_dir, \"vocab.pkl\"), \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "keyed_vectors = KeyedVectors(vector_size=embedding_size)\n",
    "keyed_vectors.add_vectors(\n",
    "    keys=[vocab.get_token(index) for index in range(vocab_size)],\n",
    "    weights=(model.weight.weight.detach() + model.weight_tilde.weight.detach()).numpy())\n",
    "    \n",
    "print(\"Similarité entre woman et man:\")\n",
    "print(keyed_vectors.similarity(\"woman\", \"man\"))\n",
    "for word in [\"computer\", \"united\", \"early\"]:\n",
    "    print(f\"Mots les plus similaires de {word}:\")\n",
    "    most_similar_words = [word for word, _ in keyed_vectors.similar_by_word(word)]\n",
    "    print(most_similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234a1fa2",
   "metadata": {},
   "source": [
    "Les résultats semblent étrange mais la raison est que je n'ai utilisé qu'un tout petit dataset, en effet la taille du dataset est d'une importance capitale pour entraîner des modèles en NLP.\n",
    "\n",
    "Voici les résultats d'un autre utilisateur sur l'ensemble du dataset *(plus d'une journée d'entraînement)*:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://user-images.githubusercontent.com/65224852/188748546-000f1293-26a7-42a7-834f-6722aadc8587.png\">\n",
    "</p>\n",
    "\n",
    "## Exploration de topics: t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "<p align=\"center\"> <i>Source de cette partie: <a href=\"https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf\">https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf</a></i></p>\n",
    "\n",
    "t-SNE est un algorithme d'apprentissage non supervisé permettant d'analyser des données décrites dans des espaces à forte dimensionnalité pour les représenter dans des espaces à deux ou trois dimensions, cet algorithme facilite la visualisation de données.\n",
    "\n",
    "Plus précisément cet algorithme trouve des patterns dans les données basé sur la similarité des représentations vectorielles (embeddings): les données proches dans l'espace original ont une probabilité élevée d'avoir une représentation proche dans le nouvel espace, dans le cadre du NLP on appelle ces regroupements des **topics**.\n",
    "\n",
    "t-SNE calcule donc une mesure de similarité entre des paires d'instances (ici mots) dans un espace à forte dimensionnalité et à la fois dans un espace à faible dimensionnalité, qu'il essaie ensuite d'optimiser à l'aide d'une fonction de coût.\n",
    "\n",
    "L'objectif est de déterminer la probabilité que des points *(voir les vecteurs comme des points)* dans un espace donné choisissent d'autres points comme voisin afin d'obtenir la distribution de probabilités des voisins dans les deux espaces et de les rendres aussi similaire que possible en minimisant leur **Divergence de Kullback-Leibler** (KLD).\n",
    "\n",
    "Soit:\n",
    "* $x_i$: vecteur représentatif d'un mot i dans l'espace à forte dimensionalité.\n",
    "* $y_i$: vecteur représentatif d'un mot i dans l'espace à faible dimensionalité.\n",
    "* $p_{j|i}$: probabilité d'obtenir un voisin j par rapport à i dans l'espace à forte dimensionalité.\n",
    "* $q_{j|i}$:probabilité d'obtenir un voisin j par rapport à i dans l'espace à faible dimensionalité.\n",
    "\n",
    "On pose:\n",
    "\n",
    "$$\\begin{align} p_{j|i} &= \\frac{exp(-\\|x_i - x_j\\|^2 \\, / \\, 2\\sigma_i^2)}{\\sum_{k \\not= i} exp(-\\|x_i - x_j\\|^2 \\, / \\, 2\\sigma_i^2)} \\qquad p_{i|i} = 0\n",
    "\\\\ q_{j|i} &= \\frac{exp(-\\|y_i - y_j\\|^2)}{\\sum_{k \\not= i} exp(-\\|y_i - y_j\\|^2)} \\qquad q_{i|i} = 0 \\end{align}$$\n",
    "\n",
    "Où:\n",
    "* $\\sigma_i$: variance du Gaussian centrée sur le point $x_i$, on la trouve en effectuant une recherche binaire.\n",
    "\n",
    "### Paranthèse sur le Gaussian:\n",
    "\n",
    "Le Gaussian *(dont la représentation est la courbe de Gauss)* est une fonction de la forme $f(x) = exp(-x^2)$ auquel on ajoute des extensions paramétriques:\n",
    "\n",
    "$$f(x) = \\alpha \\, exp\\left(-\\frac{(x - \\beta)^2}{2c^2}\\right)$$\n",
    "\n",
    "Avec:\n",
    "* $\\alpha$ et $\\beta$: constantes réelles non nulles.\n",
    "* $\\alpha$: hauteur du sommet de la courbe.\n",
    "* $\\beta$: position du centre du sommet de la courbe.\n",
    "* $c$: déviation stantard, largeur de la courbe.\n",
    "\n",
    "Le Gaussian est souvent utilisé pour représenter la densité de probabilités de variables aléatoires normalement distribuées, dans ce cas la fonction sera de la forme:\n",
    "\n",
    "$$y(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\, exp\\left(-\\frac{1}{2} \\left(\\frac{x-m}{\\sigma}\\right)^2\\right)$$\n",
    "\n",
    "Avec:\n",
    "* $m = \\beta$\n",
    "* $\\sigma^2 = c^2$\n",
    "\n",
    "<p align=\"center\"> <b>Représentation du Gaussian</b>\n",
    "<img src=\"https://user-images.githubusercontent.com/65224852/188928348-ee388eaa-9fbf-4043-b952-d04b9f415369.jpg\">\n",
    "</p>\n",
    "\n",
    "### Divergence de Kullback-Leibler:\n",
    "\n",
    "Si l'ensemble des points $y_i$ et $y_j$ modélisent correctement la similarité entre les données de haute dimension $x_i$ et $x_j$, les probabilités conditionnelles $p_{j|i}$ et $q_{j|i}$ seront égales.\n",
    "\n",
    "Une manière de mesurer l'écart entre le modèle $q_{j|i}$ et $p_{j|i}$ est d'utiliser la divergence de Kullback-Leibler *(qui dans ce cas est égal à la cross-entropy plus une constante additive)*.\n",
    "\n",
    "t-SNE minimise la divergences de Kullback-Leibler sur tous les points en utilisant une descente de gradiant, la fonction de coût C est la suivante:\n",
    "\n",
    "$$C = KL(P | Q) = \\sum_i \\sum_j p_{i|j} log \\frac{p_{i|j}}{q_{i|j}}$$\n",
    "\n",
    "Où:\n",
    "* $P$: distribution de probabilités jointes de l'ensemble des points $x$.\n",
    "* $Q$: distribution de probabilités jointes de l'ensemble des points $y$.\n",
    "\n",
    "Il reste encore des points à élucider mais vous avez l'idée générale de comment fonctionne l'algorithme t-SNE.\n",
    "\n",
    "### Mise en pratique:\n",
    "\n",
    "Dans cette mise en pratique nous visualiserons la représentation de mots sous forme de points en 2 dimensions, pour cela nous utiliserons une version pré-entraînée de GloVe et en particulier la liste des vecteurs que cette version a calculée.\n",
    "\n",
    "De plus Scikit-learn a la gentillesse de nous fournir une fonction t-SNE, c'est pourquoi nous l'utiliserons, pour installer les dépendences tapez:\n",
    "\n",
    "```\n",
    "pip install sklearn\n",
    "pip install seaborn\n",
    "pip install pandas\n",
    "pip install keras\n",
    "pip install scipy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cd7d7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 des mots similaires à 'livre':\n",
      "['books', 'author', 'published', 'novel', 'wrote', 'memoir', 'describes', 'biography', 'illustrated', 'autobiography']\n",
      "\n",
      "Distance euclidienne entre le mot 'livre' et 'nouvelle': 4.99204683303833\n",
      "Distance euclidienne entre le mot 'livre' et 'pomme': 8.160760879516602\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import spatial\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "f = open('glove.6B.300d.txt', encoding='utf-8') #Peut être téléchargé sur: https://nlp.stanford.edu/data/glove.6B.zip\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = value = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "def find_similar_word(emmbedes):\n",
    "    nearest = sorted(embeddings_index.keys(), key=lambda word: spatial.distance.euclidean(embeddings_index[word], emmbedes))\n",
    "    return nearest\n",
    "\n",
    "print(\"Top 10 des mots similaires à 'livre':\")\n",
    "print(find_similar_word(embeddings_index['book'])[1:11])\n",
    "\n",
    "print(\"\\nDistance euclidienne entre le mot 'livre' et 'nouvelle':\", spatial.distance.euclidean(embeddings_index['book'], embeddings_index['novel']))\n",
    "print(\"Distance euclidienne entre le mot 'livre' et 'pomme':\",spatial.distance.euclidean(embeddings_index['book'], embeddings_index['apple']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c12ef0",
   "metadata": {},
   "source": [
    "Puisque la distance entre un livre et une nouvelle est proche alors on devine que l'algorithme t-SNE les représentera relativement proche dans l'espace à faible dimension.\n",
    "\n",
    "Par contre le livre et la pomme étant deux notions totalement différentes, ils devraient être représenté comme éloignés.\n",
    "\n",
    "Maintenant prenons une sélection de 10 mots et observons leur répartition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48360173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seren\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:800: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Seren\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:810: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x250f8e2e770>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_components = 2 #dimension du résultat\n",
    "\n",
    "Y = ['book', 'novel', 'autobiography', 'apple', 'cat', 'human', 'car', 'game', 'airplane', 'ecology']\n",
    "X = []\n",
    "for word in Y:\n",
    "    X.append(embeddings_index[word])\n",
    "X = np.array(X)\n",
    "print(X.shape)\n",
    "\n",
    "tsne = TSNE(n_components, perplexity=6)\n",
    "tsne_result = tsne.fit_transform(X)\n",
    "tsne_result.shape\n",
    "\n",
    "tsne_result_df = pd.DataFrame({'tsne_1': tsne_result[:,0], 'tsne_2': tsne_result[:,1], 'label': Y})\n",
    "fig, ax = plt.subplots(1)\n",
    "sns.scatterplot(x='tsne_1', y='tsne_2', hue='label', data=tsne_result_df, ax=ax,s=120)\n",
    "lim = (tsne_result.min()-5, tsne_result.max()+5)\n",
    "ax.set_xlim(lim)\n",
    "ax.set_ylim(lim)\n",
    "ax.set_aspect('equal')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e54812",
   "metadata": {},
   "source": [
    "Cette répartion semble bonne pour plusieurs raisons:\n",
    "* On observe que les mots \"livre\", \"nouvelle\" et \"autobiographie\" sont rassemblés en bas à gauche ce qui est logique puisqu'ils sont corrélés.\n",
    "* On observe aussi que les mots \"voiture\" et \"avion\" sont proche dans le coin supérieur.\n",
    "* Enfin les mots \"écologie\" et \"voiture\" ou \"avion\" sont opposés, en effet la voiture et l'avion polluent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa59f0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
