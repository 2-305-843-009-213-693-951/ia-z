## BERT

BERT est un mod√®le de repr√©sentation du langage d√©velopp√© par Google AI Language.

## Mod√®le de repr√©sentation du langage

Un mod√®le de repr√©sentation du langage transforme une phrase en une repr√©sentation abstraite de la phrase qui peut √™tre utilis√©e pour une vari√©t√© de t√¢ches :

* Reconnaissance d'entit√©s nomm√©es : √âtant donn√© une phrase, classer les mots de la phrase (en choisissant parmi un ensemble de labels pr√©d√©finis).
* R√©ponse √† une question (t√¢che de classification binaire) : √âtant donn√© une question et une phrase, d√©terminez si la phrase r√©pond √† la question.
* R√©ponse √† une question (traditionnelle) : √âtant donn√© une question, trouver dans un corpus textuel la phrase qui r√©pond √† la question (marquer son d√©but et sa fin).
* Analyse de sentiment : √âtant donn√© une phrase, d√©terminer le score du sentiment (score faible = tristesse, score √©lev√© = heureux).
* Acceptabilit√© linguistique : √âtant donn√© une phrase, d√©terminer s'il s'agit d'une phrase linguistiquement acceptable.
* √âquivalence s√©mantique : pour deux phrases, d√©terminez si elles sont s√©mantiquement √©quivalentes.

## Objectifs du mod√®le BERT

1. Pr√©-entra√Æner un mod√®le de repr√©sentation du langage qui peut √™tre facilement ajust√© pour une vari√©t√© de t√¢ches.  
2. Rendre le transfer-learning pour le NLP aussi accessible que le transfer-learning pour le Computer Vision (le "ImageNET" du langage naturel).

## Utilisation de BERT pour une t√¢che sp√©cifique


Pour utiliser BERT pour une t√¢che sp√©cifique, une couche de sortie sp√©cifique √† la t√¢che doit √™tre ajout√©e √† l'architecture de base de BERT.

### Fonctionnement de BERT : 

1. Input BERT : Une phrase = s√©quence de tokens.
2. Output BERT : un √©tat cach√© final par token en input.

### Couche de sortie suppl√©mentaire sp√©cifique √† la t√¢che :

1. Classification au niveau des phrases (par exemple, analyse des sentiments). Ajoutez une couche finale avec K neurones correspondant aux K classes possibles.

2. Classification au niveau du token (par exemple, la reconnaissance des entit√©s nomm√©es), similaire √† la classification au niveau de la phrase, sauf qu'au lieu de faire une seule pr√©diction en utilisant le token masqu√© [CLS], une pr√©diction est faite pour chacun des √©tats cach√©s finaux correspondant aux labels associ√©s aux diff√©rents tokens.

## Quelle est la particularit√© de BERT ?

Il utilise un mod√®le de repr√©sentation du langage profond bidirectionnel.

### Diff√©rentes approches de la repr√©sentation du mod√®le de langage :

1. Sans contexte :  
    * Chaque mot d'une phrase est transform√© en un embedding de mots qui est ind√©pendant du contexte (c.f word2vec ou GloVe).

       La repr√©sentation sans contexte d'une phrase est une s√©quence d'embeddings de mots.

       Le mot "avocat" dans la phrase "Je mange un avocat" et le mot "avocat" dans la phrase "Je vais faire appel √† mon avocat" ont la m√™me repr√©sentation par embedding de mots.

2. Contextuel :  

    * Unidirectionnelle : La repr√©sentation d'une phrase d√©pend du contexte ("de gauche √† droite")
       La repr√©sentation contextuelle de chaque mot dans une phrase est impact√©e par les mots pr√©c√©dents mais pas par les mots suivants.
    * Bi-directionnel peu profond : La repr√©sentation d'une phrase est d√©pendante du contexte.
       La repr√©sentation unidirectionnelle de gauche √† droite d'une phrase est combin√©e avec la repr√©sentation unidirectionnelle de droite √† gauche pour former la repr√©sentation finale.
    * BERT (Deep Bi-directional) : La repr√©sentation d'une phrase d√©pend du contexte.
       Tous les mots (tokens) d'une phrase sont consid√©r√©s en m√™me temps.
       La repr√©sentation contextuelle de chaque mot de la phrase est influenc√©e par les mots pr√©c√©dents ET les mots suivants. Maaaagique ! ü™Ñüßô
    

### Comment former un mod√®le de langage bidirectionnel profond ?

### Fonctionnement global

Le mod√®le BERT repose sur deux proc√©d√©s lors de sa phase de pr√©-entrainement : *Next Sentence Prediction* (NSP) et Mask Language Modeling (MLM) que nous pr√©senterons par la suite.

Bien que le NSP (et le MLM) soient utilis√©s pour pr√©-entra√Æner les mod√®les BERT, nous pouvons utiliser ces m√©thodes exactes pour affiner nos mod√®les afin de mieux comprendre le style sp√©cifique de la langue dans des uses cases pr√©cis.

### Optimiser par rapport √† une fonction de perte :

La fonction perte de BERT = fonction de perte MLM + fonction de perte NSP.

### Fonction de perte pour l'entra√Ænement d'un mod√®le de langage traditionnel

Cas unidirectionnel :

* Premi√®re √©tape

  (Contexte) Le viewer_1 est [?] satisfait mais le viewer_2 est m√©content. 
  
  T√¢che de classification : pr√©dire [?] en tenant compte du contexte comme entr√©e.

* √âtape 2

  Le viewer_1 (Contexte) est satisfait [?] mais le viewer_2 est m√©content.

  La fonction de perte est la vraisemblance moyenne de cette classification.

### Fonction de perte (MLM)

Cas profond bidirectionnel (BERT) :

* (Contexte) Le viewer_1 est [?] [Token masqu√©] mais le viewer_2 est m√©content. 

* T√¢che de classification : pr√©dire la cible avec le contexte en entr√©e.

* Un token choisi au hasard est masqu√© et le mod√®le essaie de retrouver le token en se basant sur le contexte.

* La fonction de perte est la probabilit√© moyenne de la classification MLM.

### Pr√©diction de la phrase suivante (NSP) :

* Choisir deux phrases dans le corpus et les concat√©ner.
* Les phrases concat√©n√©es sont donn√©es en entr√©e au mod√®le.
* Etiquette NSP = 1 si B suit A dans le corpus de texte
* Label NSP = 0 si B est une phrase choisie au hasard dans le corpus. 
* L'ensemble d'entra√Ænement est construit de telle sorte qu'il y ait 50% de chaque √©tiquette.
* T√¢che de classification binaire : d√©terminer si B suit A ou non.
* Calculer le score de classification (= score NSP) 

## Liens int√©ressants : 

## Tutoriels :
* https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb

* Publication originale : https://arxiv.org/abs/1810.04805
* https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270
* https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html
* https://lesdieuxducode.com/blog/2019/4/bert--le-transformer-model-qui-sentraine-et-qui-represente