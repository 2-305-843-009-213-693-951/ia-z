{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbTUCiuA7iv8"
   },
   "source": [
    "# Clustering\n",
    "\n",
    "Si vous vous rappelez bien du chapitre \"Eléments de définition\", il y a trois types principaux de Machine Learning; Supervisé, non supervisé et par renforcement. Jusqu'à maintenant nous n'avions que des cas d'apprentissage supervisé (nous avions les labels), il est donc temps de parler de l'apprentissage non-supervisé (nous n'avons pas les labels). \n",
    "\n",
    "Imaginons que nous avons des images de chiens et de chats qui ne sont pas étiquettées. Une solution serait de payer des gens pour ajouter les labels, mais elle est couteuse en temps et en argent. Une autre est d'utiliser du clustering. Cette approche va essayer de créer des groupes (clusters) en utilisant les données. \n",
    "\n",
    "Une idée serait de partir du principe que les données de même classe (chiens et chats) sont proches dans l'espace définit par les pixels de l'image. Pour les regrouper, nous pourrions avoir deux points, un pour les chiens et un pour les chats. Nous classifierons l'image en fonction du point duquel elle est le plus proche. La question est donc: Comment trouver ces points? \n",
    "\n",
    "La réponse est simple, en utilsant Kmeans!\n",
    "\n",
    "## Kmeans\n",
    "\n",
    "L'algorithm se base sur ce qu'on appel des centroids, ce sont des points qui seront placé au milieu des clusters. Chaque échantillon fait partie de la catégorie dont le centroid est le plus proche. La procédure se passe en deux parties\n",
    "1. Attribution des échantillons aux clusters\n",
    "2. Mise à jour des centroids\n",
    "\n",
    "### En détail\n",
    "Afin de nous faciliter la tâche, nous allons introduire une matrice binaire $z$ qui est de taille $N\\times K$, $N$ détermine le nombre de points de données et $K$ le nombre de clusters que nous cherchons. De cette façon, l'entrée $z_{1,1}=0$ voudra dire que $x_1$ n'est pas dans le cluster 1, et $z_{1,2}=1$ représente l'appartenance du point $x_1$ au cluster 2. Nous avons donc la contrainte qu'il ne peut y avoir que une entrée qui vaut 1 et le reste doit être 0, mathématiquement, cela donne $\\sum_k z_{n,k} = 1$. \n",
    "<br> Maintenant que cela est clair, nous pouvons (enfin) décrire l'algorithm.\n",
    "\n",
    "### L'algorithme\n",
    "1. On initialise les centroids $\\mu_n$. Pour l'instant nous le faisons au hasard. \n",
    "2. On attribue le cluster de chaque données $x_n$ au centroid le plus proche et remplissons la matrice $z$.<br> Comme avec Knn. nous pouvons prendre la mesure de distance qui nous convient le mieux. Dans notre cas nous utiliserons l'euclidienne. \n",
    "3. On met les centroids à jour en calculant la moyenne des points présent dans le cluster\n",
    "$$\\mu_k = \\frac{\\sum_{n=1}^N z_{nk}\\textbf{x}_n}{\\sum_{n=1}^N z_{nk}}$$\n",
    "4. On retourne au point 2, jusqu'à ce que $z$ ne change plus. \n",
    "\n",
    "Ce problème peut-être décrit par une fonction de couts que nous voulons minimiser:\n",
    "\t\t\t\n",
    "$$min\\sum_n\\sum_k z_{n,k} (x_n-\\mu_k)^T(x_n-\\mu_k)$$\n",
    "$$\\textbf{s.t.: } z_{n,k}\\in \\{0,1\\} \\text{ et } \\sum_k z_{n,k} = 1 $$\n",
    "\n",
    "Le but est de rendre la somme des distances la plus petite possible. Kmeans va converger à chaque fois mais vu que les centroids sont initilisés aléatoirement et que le problême n'est pas convex (une seule solution) chaque fois que nous lançons Kmeans nous aurons potentiellement une solution différente.\n",
    "\n",
    "### Soucis et Solutions\n",
    "Il y a deux problèmes majeurs, comment déterminer le nombre de clusters. Dans certains cas nous avons énorméments de données sans labels et il est impossible de déterminer le nombre de cluster présent. Le deuxième est l'initialisation des centroids. Vu que l'initilisation a un impact majeur sur la solution finale, il faut le faire de manière intelligent sans risquer de trop biaiser le résultat. \n",
    "\n",
    "#### Initlisation des centroids\n",
    "Il existe plusieurs approches plus ou moins efficaces comme l'initilisation aléatoire dans une certaine \"zone\", prendre $K$ points dans les données et utiliser leur position comme centroid. Dans les deux cas, nous avons potentiellement des centroids très proche l'un de l'autre, ce qui ralentit la convergence. L'approche la plus efficace (à ce jour) se nomme kmeans++ et est celle utilisée par la libraire Scikit-Learn. \n",
    "\n",
    "L'idée est assez simple, nous choisissons un point dans $X$ aléatoirement et utilisons ses coordonnées comme coordonnées pour $\\mu_1$. Ensuite nous trouvons le point le plus éloigné de $\\mu_1$ et l'utilisons pour déterminer $\\mu_2$. Puis, nous trouvons le points le plus éloigné de $\\mu_1$ et $\\mu_2$, etc. \n",
    "\n",
    "Cette approche nous donne une approximation $O(logK)$ de la réponse finale. \n",
    "\n",
    "#### Choix du nombre de cluster\n",
    "Il existe plusieurs approches, celle décrite ici est basé sur ce qu'on appele la variance intra-cluster. La variance d'un cluster est déterminé par:\n",
    "$$W_k:=\\frac{1}{|\\mu_k|}\\sum_{x\\in \\mu_k}(x-\\mu_k)^2$$\n",
    "La variance total est la somme de tous les $W_k$. \n",
    "\n",
    "Le but est de choisir le nombre de cluster $K$ qui minimise la variance intra-cluster.\n",
    "\n",
    "## À savoir\n",
    "Étant donnée que nous n'avons pas les labels, il n'est, la plupart du temps, pas possible d'évaluer un algorithm de clustering. Il faudra soit lui faire confiance ou regarder par soit même. De plus, on sait que Kmeans va toujours converger, il va donc toujours donner une infomation. Dans le cas des images de chien et de chat, il trouvera autant de cluster qu'il y a d'images si on le lui demande. Il faut donc être prudent lors de l'utilisation de l'algorithm. \n",
    "\n",
    "### À retenir\n",
    "\n",
    "- d\n",
    "\n",
    "## Implémentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 812
    },
    "id": "P6wYcH6n7Uid",
    "outputId": "11605812-3ddd-4dbe-a17d-8168175385bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5555255  0.57978013]\n",
      " [0.20073797 0.3382875 ]]\n",
      "[[0.58811734 0.60681782]\n",
      " [0.24004356 0.35860791]]\n",
      "[[0.60088987 0.62040944]\n",
      " [0.25526379 0.36440545]]\n",
      "[[0.61470668 0.63285889]\n",
      " [0.26909674 0.37252295]]\n",
      "[[0.61470668 0.63285889]\n",
      " [0.26909674 0.37252295]]\n",
      "Final:\n",
      " [[0.61470668 0.63285889]\n",
      " [0.26909674 0.37252295]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_4.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "-- Création de l'ensemble de donnée --\n",
    "Notez que j'utilise une distribution normal ce qui permet d'avoir tous les points d'une même class relativement proche.\n",
    "De plus, je génère un ensemble balancé\n",
    "'''\n",
    "\n",
    "elements = 50\n",
    "random_state = 42\n",
    "K = 5\n",
    "\n",
    "def generate_data(elements, random_state):\n",
    "    rng=np.random.RandomState(random_state)\n",
    "\n",
    "    classes_x = rng.normal(0, 0.55, (2,elements))\n",
    "    classes_y = rng.normal(1, 0.55, (2,elements))\n",
    "    labels = np.concatenate((np.zeros(elements), np.ones(elements)))\n",
    "    dset = np.zeros((2,elements*2))\n",
    "\n",
    "    dset[0] = np.concatenate((classes_x[0], classes_y[0]))\n",
    "    dset[1] = np.concatenate((classes_x[1], classes_y[1]))\n",
    "\n",
    "    return dset, labels\n",
    "\n",
    "class Kmeans:\n",
    "    def __init__(self, K):\n",
    "        self.K = K\n",
    "        self.centroids = None\n",
    "\n",
    "    def fit(self, data):\n",
    "        self.centroids = np.random.uniform(np.min(data), np.max(data), size=(self.K, len(data[0])))\n",
    "        #print(self.centroids)\n",
    "\n",
    "        prev_z = None\n",
    "        curr_z = None\n",
    "      \n",
    "        while type(prev_z) == type(None) or (prev_z != curr_z).any():\n",
    "            prev_z = curr_z\n",
    "            curr_z = np.zeros((len(data), self.K))\n",
    "            #print(f\"Prev_z: {prev_z}\")\n",
    "            #print(f\"Curr_z: {curr_z}\")\n",
    "\n",
    "            for ind_point, dpoint in enumerate(data):\n",
    "                dist = np.zeros(self.K)\n",
    "\n",
    "                for ind_centr, centroid in enumerate(self.centroids):\n",
    "                    dist[ind_centr] = self.distance(dpoint, centroid)                \n",
    "                curr_z[ind_point][np.argmin(dist)] = 1\n",
    "            \n",
    "            for ind_centr, centroid in enumerate(self.centroids): \n",
    "                new_centr = np.zeros(len(centroid))\n",
    "\n",
    "                for dim in range(len(centroid)):\n",
    "                    new_centr[dim] = np.dot(curr_z[:, ind_centr],data[:,dim])/sum(curr_z[:, ind_centr])\n",
    "\n",
    "                self.centroids[ind_centr] = new_centr\n",
    "            \n",
    "            print(self.centroids)\n",
    "\n",
    "    def kmeans_pp(self, data):\n",
    "        pass\n",
    "    \n",
    "\n",
    "    def predict(self, data_point):\n",
    "        preds = []\n",
    "\n",
    "        for dp in data_point:\n",
    "            distances = []\n",
    "\n",
    "            for point in self.centroids:\n",
    "                distances.append(self.distance(dp, point))\n",
    "\n",
    "            #sorted_distances = np.argsort(distances)\n",
    "            preds.append(np.argmin(distances))\n",
    "        \n",
    "        return preds\n",
    "        \n",
    "\n",
    "    def distance(self, x_new, x):\n",
    "        return np.power(np.dot(x_new-x, x_new-x), 2)\n",
    "\n",
    "data, label = generate_data(elements, random_state)\n",
    "\n",
    "data[0,:] = (data[0,:]-min(data[0,:]))/(max(data[0,:])-min(data[0,:]))\n",
    "data[1,:] = (data[1,:]-min(data[1,:]))/(max(data[1,:])-min(data[1,:]))\n",
    "\n",
    "# print(data)\n",
    "\n",
    "kmeans = Kmeans(2)\n",
    "\n",
    "kmeans.fit(data.T)\n",
    "\n",
    "print(f\"Final:\\n {kmeans.centroids}\")\n",
    "\n",
    "preds = []\n",
    "[preds.append(str(pred)) for pred in kmeans.predict(data.T)]\n",
    "\n",
    "'''\n",
    "-- Plot --\n",
    "'''\n",
    "\n",
    "fig0 = px.scatter(x=data[0,:], y=data[1,:], color=preds, color_discrete_sequence=[\"rgb(2,48,71)\", \"rgb(255,158,2)\"])\n",
    "fig1 = px.scatter(x=kmeans.centroids.T[0,:], y=kmeans.centroids.T[1,:], color=[\"Centroid 0\",\"Centroid 1\"], symbol_sequence=[\"square\"], color_discrete_sequence=[\"rgb(255,158,2)\", \"rgb(2,48,71)\"])\n",
    "\n",
    "fig1.update_traces(marker=dict(size=10,\n",
    "                              line=dict(width=2,\n",
    "                                        color='DarkSlateGrey')),\n",
    "                  selector=dict(mode='markers'))\n",
    "\n",
    "fig2 = go.Figure(data=fig0.data + fig1.data)\n",
    "fig2.show('iframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "cluster_cours.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
